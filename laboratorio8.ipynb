{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8086f3be-c8f0-4720-8969-149fa54e9143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Laboratorio 8 - Transformaciones con Spark\n",
    "\n",
    "#### Edwin Ortega 22305\n",
    "#### Esteban Zambrano 22119\n",
    "#### Diego García 22404"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52c12913-162e-413d-b52f-4c24f27629a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Carga de Datos y Análisis Exploratorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8122b58-bde9-4955-90c4-f769567bb706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "##### Instalación a tener en cuenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a87e085-0922-4fb9-a808-b3210d6b6c40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85368396-49dd-4df4-8243-793560a99858",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Convertir Excel a un CSV por hoja\n",
    "##### Rutas, imports y utilidades "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "323dd6c1-c628-44db-9473-580f123d4c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Rutas\n",
    "SOURCE_XLSX = \"/Volumes/workspace/default/lab8/Principales_resultados_PNC_24.xlsx\"\n",
    "\n",
    "# Detectar carpeta del repo\n",
    "ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "nb_path = ctx.notebookPath().get()\n",
    "parts   = nb_path.split(\"/\")\n",
    "# raíz del repo\n",
    "REPO_ROOT = \"/Workspace/\" + \"/\".join(parts[:4])\n",
    "OUT_DIR   = os.path.join(REPO_ROOT, \"Data\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Funciones auxiliares\n",
    "def slug(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \"_\", s.strip())\n",
    "    s = s.replace(\"á\",\"a\").replace(\"é\",\"e\").replace(\"í\",\"i\").replace(\"ó\",\"o\").replace(\"ú\",\"u\").replace(\"ñ\",\"n\")\n",
    "    s = re.sub(r\"[^A-Za-z0-9_]+\", \"\", s)\n",
    "    return s.lower()\n",
    "\n",
    "def clean_number(x):\n",
    "    # Convierte '1,234', '-', '–', '' a numérico\n",
    "    if pd.isna(x): return np.nan\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip()\n",
    "        if x in {\"-\", \"–\", \"—\", \"\"}: return 0\n",
    "        x = x.replace(\",\", \"\")\n",
    "        # algunos libros traen espacios finos o NBSP\n",
    "        x = x.replace(\"\\u202f\",\"\").replace(\"\\xa0\",\"\")\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Palabras clave para encontrar la fila de encabezados útil\n",
    "HEADER_HINTS = [\n",
    "    \"Departamento\", \"Mes de ocurrencia\", \"Día de la semana\", \"Dia de la semana\",\n",
    "    \"Día de ocurrencia\", \"Dia de ocurrencia\", \"Hora de ocurrencia\",\n",
    "    \"Tipo de accidente\", \"Año de ocurrencia\", \"Ano de ocurrencia\"\n",
    "]\n",
    "\n",
    "MESES = [\"Enero\",\"Febrero\",\"Marzo\",\"Abril\",\"Mayo\",\"Junio\",\"Julio\",\"Agosto\",\"Septiembre\",\"Octubre\",\"Noviembre\",\"Diciembre\"]\n",
    "DIAS  = [\"Lunes\",\"Martes\",\"Miércoles\",\"Miercoles\",\"Jueves\",\"Viernes\",\"Sábado\",\"Sabado\",\"Domingo\"]\n",
    "ANIOS = [\"2020\",\"2021\",\"2022\",\"2023\",\"2024\"]\n",
    "TIPOS = [\"Colisión\",\"Colision\",\"Colisi\\u00f3n\",\"Atropello\",\"Derrape\",\"Choque\",\"Vuelco\",\"Embarrancó\",\"Embarranco\",\"Encuentó\",\"Encuento\",\"Caída\",\"Caida\",\"Ignorado\"]\n",
    "\n",
    "NO_TABULARES = {\"Portada-Presentación\", \"Índice\", \"Indice\", \"Directorio\", \"Ficha técnica\", \"Nota técnica\"}\n",
    "\n",
    "def find_header_row(raw_df: pd.DataFrame):\n",
    "    # busca la primera fila que contenga un hint de encabezado\n",
    "    for i, row in raw_df.iterrows():\n",
    "        joined = \" | \".join([str(x) for x in row.values if not pd.isna(x)])\n",
    "        for hint in HEADER_HINTS:\n",
    "            if hint.lower() in joined.lower():\n",
    "                return i\n",
    "    # fallback: fila donde la primera col diga \"Total\"\n",
    "    first_col = raw_df.iloc[:,0].astype(str).str.strip().str.lower()\n",
    "    idx = first_col[first_col.eq(\"total\")].index\n",
    "    if len(idx)>0:\n",
    "        # suele ser una fila de datos; probamos la fila anterior como encabezado\n",
    "        return max(0, idx[0]-1)\n",
    "    return None\n",
    "\n",
    "def normalize_columns(cols):\n",
    "    # rellena vacíos y duplicados\n",
    "    out, seen = [], {}\n",
    "    for c in cols:\n",
    "        c = \"\" if pd.isna(c) else str(c).strip()\n",
    "        c = c.replace(\"\\n\",\" \").replace(\"\\r\",\" \")\n",
    "        c = re.sub(r\"\\s+\",\" \", c)\n",
    "        if c == \"\" or c.lower().startswith(\"unnamed:\"):\n",
    "            c = \"col\"\n",
    "        # tildes coherentes\n",
    "        c = (c\n",
    "             .replace(\"Miércoles\",\"Miércoles\").replace(\"Miercoles\",\"Miércoles\")\n",
    "             .replace(\"Sábado\",\"Sábado\").replace(\"Sabado\",\"Sábado\")\n",
    "             .replace(\"Colision\",\"Colisión\").replace(\"Caida\",\"Caída\")\n",
    "             .replace(\"Embarranco\",\"Embarrancó\").replace(\"Encuento\",\"Encuentó\"))\n",
    "        # duplicados\n",
    "        base = c\n",
    "        k = seen.get(base, 0)\n",
    "        if k:\n",
    "            c = f\"{base}_{k+1}\"\n",
    "        seen[base] = k+1\n",
    "        out.append(c)\n",
    "    return out\n",
    "\n",
    "def detect_value_axis(columns):\n",
    "    cols = [str(c) for c in columns]\n",
    "    sets = {\n",
    "        \"mes\": [c for c in cols if c in MESES],\n",
    "        \"dia_semana\": [c for c in cols if c in DIAS],\n",
    "        \"anio\": [c for c in cols if c in ANIOS],\n",
    "        \"tipo_accidente\": [c for c in cols if c in TIPOS],\n",
    "    }\n",
    "    # elegir el que tenga más coincidencias\n",
    "    winner, win_len = None, 0\n",
    "    for k, v in sets.items():\n",
    "        if len(v) > win_len:\n",
    "            winner, win_len = k, len(v)\n",
    "    return winner, sets.get(winner, [])\n",
    "\n",
    "def tidy_if_possible(df: pd.DataFrame, sheet: str):\n",
    "    # convierte a long si detecta ejes conocidos\n",
    "    axis, value_cols = detect_value_axis(df.columns)\n",
    "    if axis and len(value_cols) >= 3:\n",
    "        # id_vars = todas menos las de valores y 'Total'\n",
    "        id_vars = [c for c in df.columns if c not in set(value_cols) and c != \"Total\"]\n",
    "        long_df = df.melt(id_vars=id_vars, value_vars=value_cols,\n",
    "                          var_name=axis, value_name=\"valor\")\n",
    "        # cast numérico\n",
    "        long_df[\"valor\"] = long_df[\"valor\"].map(clean_number)\n",
    "        long_df.insert(0, \"hoja\", sheet)\n",
    "        return long_df, \"long\"\n",
    "    # si no, limpiamos números en las columnas numéricas detectadas\n",
    "    for c in df.columns:\n",
    "        if c == \"Total\" or re.fullmatch(r\"\\d{4}\", str(c)) or c in MESES or c in DIAS or c in TIPOS:\n",
    "            df[c] = df[c].map(clean_number)\n",
    "    df.insert(0, \"hoja\", sheet)\n",
    "    return df, \"wide\"\n",
    "\n",
    "def is_non_tabular(name: str, raw_df: pd.DataFrame):\n",
    "    if name in NO_TABULARES:\n",
    "        return True\n",
    "    # Si prácticamente todo es NaN\n",
    "    after_drop = raw_df.dropna(how=\"all\")\n",
    "    return after_drop.shape[0] < 3 or after_drop.shape[1] < 2\n",
    "\n",
    "# Proceso por hoja\n",
    "xl = pd.ExcelFile(SOURCE_XLSX)\n",
    "resumen = []\n",
    "\n",
    "print(\"Hojas detectadas:\")\n",
    "print(\" - \" + \"\\n - \".join(xl.sheet_names))\n",
    "print()\n",
    "\n",
    "for sheet in xl.sheet_names:\n",
    "    raw = xl.parse(sheet, header=None, dtype=object)\n",
    "    if is_non_tabular(sheet, raw):\n",
    "        print(f\"SKIP hoja no tabular: {sheet}\")\n",
    "        continue\n",
    "\n",
    "    hdr = find_header_row(raw)\n",
    "    if hdr is None:\n",
    "        print(f\"(Aviso) {sheet}: no pude localizar encabezados. Exporto tal cual.\")\n",
    "        tmp = raw.dropna(how=\"all\", axis=0).dropna(how=\"all\", axis=1)\n",
    "        tmp.columns = normalize_columns(tmp.iloc[0].tolist())\n",
    "        tmp = tmp.iloc[1:].reset_index(drop=True)\n",
    "        tidy, form = tidy_if_possible(tmp, sheet)\n",
    "    else:\n",
    "        body = raw.iloc[hdr:].copy()\n",
    "        # primera fila como encabezado\n",
    "        body.columns = normalize_columns(body.iloc[0].tolist())\n",
    "        body = body.iloc[1:].reset_index(drop=True)\n",
    "        # cortar “Fuente:” si aparece\n",
    "        first_col_name = body.columns[0]\n",
    "        if first_col_name:\n",
    "            stop_idx = body[first_col_name].astype(str).str.startswith(\"Fuente\").fillna(False)\n",
    "            if stop_idx.any():\n",
    "                body = body.loc[:stop_idx.idxmax()-1]\n",
    "        # eliminar filas/cols completamente vacías\n",
    "        body = body.dropna(how=\"all\", axis=0).dropna(how=\"all\", axis=1)\n",
    "        tidy, form = tidy_if_possible(body, sheet)\n",
    "\n",
    "    # Guardar CSV\n",
    "    fname = f\"PRPNC24_{slug(sheet)}.csv\"\n",
    "    out_path = os.path.join(OUT_DIR, fname)\n",
    "    # Para escribir en /Workspace/...\n",
    "    tidy.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"OK → {sheet} -> {os.path.basename(out_path)}  ({form}, {len(tidy)} filas)\")\n",
    "\n",
    "    resumen.append({\n",
    "        \"hoja\": sheet,\n",
    "        \"csv\": os.path.basename(out_path),\n",
    "        \"formato\": form,\n",
    "        \"filas\": len(tidy)\n",
    "    })\n",
    "\n",
    "# Resumen final\n",
    "res = pd.DataFrame(resumen).sort_values([\"hoja\"])\n",
    "print(\"\\nResumen de CSV generados:\")    \n",
    "try:\n",
    "    display(res)\n",
    "except:\n",
    "    print(res.to_string(index=False))\n",
    "\n",
    "print(f\"\\nArchivos guardados en:\\n{OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f0b96fc-3146-4991-824a-8aed9a555ef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Unir por grupo (hechos, vehículos, víctimas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f76056f4-1903-4aac-8c72-b7961e2d7685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, glob, re, unicodedata, pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Rutas\n",
    "ROOT_DIR = \"/Workspace/Users/ort22305@uvg.edu.gt/Lab8_DS\" \n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"Data\") \n",
    "assert os.path.isdir(DATA_DIR), f\"No existe {DATA_DIR}. Genera antes los CSV por hoja.\"\n",
    "\n",
    "# Utilidades\n",
    "MESES = {\"Enero\",\"Febrero\",\"Marzo\",\"Abril\",\"Mayo\",\"Junio\",\"Julio\",\"Agosto\",\"Septiembre\",\"Octubre\",\"Noviembre\",\"Diciembre\"}\n",
    "DIAS  = {\"Lunes\",\"Martes\",\"Miércoles\",\"Miercoles\",\"Jueves\",\"Viernes\",\"Sábado\",\"Sabado\",\"Domingo\"}\n",
    "TIPO_ACC = {\"Colisión\",\"Colision\",\"Atropello\",\"Derrape\",\"Choque\",\"Vuelco\",\"Embarrancó\",\"Embarranco\",\"Encuentó\",\"Encuentro\",\"Caída\",\"Caida\",\"Ignorado\"}\n",
    "\n",
    "def strip_accents(s):\n",
    "    if pd.isna(s): return s\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', str(s)) if unicodedata.category(c)!='Mn')\n",
    "\n",
    "def detect_group_from_csv(path):\n",
    "    \"\"\"Lee las primeras filas del CSV y detecta el grupo por palabras clave del título.\"\"\"\n",
    "    head = pd.read_csv(path, nrows=25, header=None, dtype=str, encoding=\"utf-8-sig\")\n",
    "    txt = ' '.join([strip_accents(x) for x in head.fillna('').values.ravel().tolist()]).lower()\n",
    "    if \"vehiculos involucrados\" in txt: return \"vehiculos\"\n",
    "    if (\"fallecidos\" in txt) or (\"lesionados\" in txt): return \"victimas\"\n",
    "    if \"accidentes de transito\" in txt: return \"hechos\"\n",
    "    return \"desconocido\"\n",
    "\n",
    "def to_int_or_none(x):\n",
    "    s = str(x).strip()\n",
    "    if s in (\"\", \"nan\", \"None\", \"-\", \"–\"): \n",
    "        return None\n",
    "    # quitar separador de miles (ej. \"8,401\")\n",
    "    s = s.replace(\",\", \"\").replace(\" \", \"\")\n",
    "    try:\n",
    "        return int(float(s))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def guess_dim2_name(cols2: set):\n",
    "    \"\"\"Etiqueta de la 2ª dimensión según las columnas detectadas.\"\"\"\n",
    "    if cols2 and cols2.issubset(MESES | {\"Total\"}):\n",
    "        return \"Mes\"\n",
    "    if cols2 and cols2.issubset(DIAS | {\"Total\"}):\n",
    "        return \"Dia_semana\"\n",
    "    # muchos cuadros de tipo de accidente traen estas columnas\n",
    "    if cols2 and len(cols2 & TIPO_ACC) >= max(3, int(0.4*len(cols2))):\n",
    "        return \"Tipo_accidente\"\n",
    "    return \"Categoria\"\n",
    "\n",
    "def tidy_prpnc24(path, default_year=2024):\n",
    "    \"\"\"\n",
    "    Devuelve un DataFrame pandas en formato largo con columnas:\n",
    "    [anio, dim1, dim1_val, dim2, dim2_val, valor]\n",
    "    \"\"\"\n",
    "    raw = pd.read_csv(path, header=None, dtype=str, encoding=\"utf-8-sig\")\n",
    "    # buscar fila de encabezado real\n",
    "    candidates = {\"Departamento\",\"Mes de ocurrencia\",\"Día de la semana\",\"Hora de ocurrencia\",\n",
    "                  \"Día de ocurrencia\",\"Tipo de accidente\",\"Sexo\",\"Edad\",\"Rango de edad\"}\n",
    "    header_idx = None\n",
    "    for i in range(min(50, len(raw))):\n",
    "        row = raw.iloc[i].fillna(\"\").astype(str).str.strip()\n",
    "        if any(c in row.values for c in candidates):\n",
    "            header_idx = i\n",
    "            break\n",
    "    if header_idx is None:\n",
    "        # fallback: intenta detectar la fila anterior a la primera que contiene \"Total\"\n",
    "        for i in range(5, min(80, len(raw))):\n",
    "            if raw.iloc[i].astype(str).str.contains(\"Total\", case=False, na=False).any():\n",
    "                header_idx = i-1\n",
    "                break\n",
    "    if header_idx is None:\n",
    "        # como último recurso: usa la primera fila no vacía\n",
    "        header_idx = 0\n",
    "\n",
    "    # set columnas y recorta encabezado\n",
    "    df = raw.copy()\n",
    "    df.columns = df.iloc[header_idx].fillna(\"\").astype(str).str.strip()\n",
    "    df = df.iloc[header_idx+1:].reset_index(drop=True)\n",
    "\n",
    "    # eliminar pie (\"Fuente:\")\n",
    "    mask_fuente = df.apply(lambda r: r.astype(str).str.contains(\"Fuente\", case=False, na=False).any(), axis=1)\n",
    "    if mask_fuente.any():\n",
    "        df = df.loc[~mask_fuente].copy()\n",
    "\n",
    "    # normaliza nombres de columnas\n",
    "    df.columns = [strip_accents(c).strip() for c in df.columns]\n",
    "    # quita columnas completamente vacías\n",
    "    df = df.loc[:, (df != \"\").any(axis=0)]\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"anio\",\"dim1\",\"dim1_val\",\"dim2\",\"dim2_val\",\"valor\"])\n",
    "\n",
    "    first_col = df.columns[0]                     # p.ej. \"Departamento\" / \"Mes de ocurrencia\" / etc.\n",
    "    year_cols = [c for c in df.columns if re.fullmatch(r\"20\\d{2}\", c)]\n",
    "    # columnas para 2ª dimensión (quitar Total y la primera columna)\n",
    "    cols2 = [c for c in df.columns if c not in [first_col, \"Total\"] and not re.fullmatch(r\"20\\d{2}\", c)]\n",
    "    dim2_name = guess_dim2_name(set(cols2))\n",
    "\n",
    "    if year_cols:\n",
    "        wide = df[[first_col] + year_cols].copy()\n",
    "        out = wide.melt(id_vars=[first_col], value_vars=year_cols, var_name=\"anio\", value_name=\"valor\")\n",
    "        out[\"dim2\"] = pd.NA\n",
    "        out[\"dim2_val\"] = pd.NA\n",
    "    else:\n",
    "        # un año (2024) con columnas de meses, días, tipo_accidente, etc.\n",
    "        if not cols2:\n",
    "            # si no hay 2ª dimensión, usa solo la primera\n",
    "            tmp = df[[first_col]].copy()\n",
    "            tmp[\"valor\"] = None\n",
    "            out = tmp.rename(columns={first_col: \"dim1_val\"})\n",
    "            out[\"dim2\"] = pd.NA\n",
    "            out[\"dim2_val\"] = pd.NA\n",
    "        else:\n",
    "            long = df.melt(id_vars=[first_col], value_vars=cols2,\n",
    "                           var_name=\"dim2_val\", value_name=\"valor\")\n",
    "            out = long.rename(columns={first_col: \"dim1_val\"})\n",
    "            out[\"dim2\"] = dim2_name\n",
    "        out[\"anio\"] = default_year\n",
    "\n",
    "    out[\"dim1\"] = first_col\n",
    "    out[\"valor\"] = out[\"valor\"].apply(to_int_or_none)\n",
    "    out = out.dropna(subset=[\"valor\"]).reset_index(drop=True)\n",
    "    # ordenar columnas\n",
    "    out = out[[\"anio\",\"dim1\",\"dim1_val\",\"dim2\",\"dim2_val\",\"valor\"]]\n",
    "    return out\n",
    "\n",
    "# Procesar todos los PRPNC24_cuadro_*.csv\n",
    "csv_paths = sorted(glob.glob(os.path.join(DATA_DIR, \"PRPNC24_cuadro_*.csv\")))\n",
    "registros = []\n",
    "skipped = []\n",
    "\n",
    "for p in csv_paths:\n",
    "    grupo = detect_group_from_csv(p)\n",
    "    if grupo == \"desconocido\":\n",
    "        skipped.append(os.path.basename(p))\n",
    "        continue\n",
    "    # número de cuadro\n",
    "    m = re.search(r\"cuadro_(\\d+)\\.csv$\", os.path.basename(p))\n",
    "    cuadro_num = int(m.group(1)) if m else None\n",
    "\n",
    "    # algunos cuadros son serie 2020-2024 (4 y 6). Para el resto usamos 2024.\n",
    "    default_year = 2024\n",
    "    if cuadro_num in {4, 6}:\n",
    "        default_year = None\n",
    "\n",
    "    tidy = tidy_prpnc24(p, default_year=2024 if default_year is None else default_year)\n",
    "    if tidy.empty:\n",
    "        skipped.append(os.path.basename(p))\n",
    "        continue\n",
    "    tidy[\"grupo\"]  = grupo\n",
    "    tidy[\"cuadro\"] = cuadro_num\n",
    "    tidy[\"csv\"]    = os.path.basename(p)\n",
    "    registros.append(tidy)\n",
    "\n",
    "# Concatenar todo y separar por grupo\n",
    "all_long = pd.concat(registros, ignore_index=True) if registros else pd.DataFrame(\n",
    "    columns=[\"anio\",\"dim1\",\"dim1_val\",\"dim2\",\"dim2_val\",\"valor\",\"grupo\",\"cuadro\",\"csv\"]\n",
    ")\n",
    "\n",
    "hechos_pd    = all_long[all_long[\"grupo\"]==\"hechos\"].reset_index(drop=True)\n",
    "vehiculos_pd = all_long[all_long[\"grupo\"]==\"vehiculos\"].reset_index(drop=True)\n",
    "victimas_pd  = all_long[all_long[\"grupo\"]==\"victimas\"].reset_index(drop=True)\n",
    "\n",
    "# Spark\n",
    "hechos_long_s  = spark.createDataFrame(hechos_pd)    if not hechos_pd.empty    else spark.createDataFrame([], \"anio string, dim1 string, dim1_val string, dim2 string, dim2_val string, valor int, grupo string, cuadro int, csv string\")\n",
    "vehic_long_s   = spark.createDataFrame(vehiculos_pd) if not vehiculos_pd.empty else spark.createDataFrame([], \"anio string, dim1 string, dim1_val string, dim2 string, dim2_val string, valor int, grupo string, cuadro int, csv string\")\n",
    "victims_long_s = spark.createDataFrame(victimas_pd)  if not victimas_pd.empty  else spark.createDataFrame([], \"anio string, dim1 string, dim1_val string, dim2 string, dim2_val string, valor int, grupo string, cuadro int, csv string\")\n",
    "\n",
    "hechos_long_s.createOrReplaceTempView(\"hechos_long\")\n",
    "vehic_long_s.createOrReplaceTempView(\"vehiculos_long\")\n",
    "victims_long_s.createOrReplaceTempView(\"victimas_long\")\n",
    "\n",
    "print(f\"[RESUMEN] filas -> hechos: {hechos_long_s.count():,} | vehiculos: {vehic_long_s.count():,} | victimas: {victims_long_s.count():,}\")\n",
    "print(\"Saltadas por forma no reconocida:\", skipped[:10], (\"...\" if len(skipped)>10 else \"\"))\n",
    "\n",
    "# Muestras\n",
    "print(\"\\n[hechos_long] ejemplo:\")\n",
    "display(hechos_long_s.limit(15))\n",
    "\n",
    "print(\"\\n[vehiculos_long] ejemplo:\")\n",
    "display(vehic_long_s.limit(15))\n",
    "\n",
    "print(\"\\n[victimas_long] ejemplo:\")\n",
    "display(victims_long_s.limit(15))\n",
    "\n",
    "# Guardar también como CSV 'planos' para Git/GitHub\n",
    "out_hechos    = os.path.join(DATA_DIR, \"hechos_long.csv\")\n",
    "out_vehiculos = os.path.join(DATA_DIR, \"vehiculos_long.csv\")\n",
    "out_victimas  = os.path.join(DATA_DIR, \"victimas_long.csv\")\n",
    "hechos_pd.to_csv(out_hechos, index=False, encoding=\"utf-8-sig\")\n",
    "vehiculos_pd.to_csv(out_vehiculos, index=False, encoding=\"utf-8-sig\")\n",
    "victimas_pd.to_csv(out_victimas, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"\\nGuardados CSV normalizados en Data/:\")\n",
    "print(\" -\", out_hechos)\n",
    "print(\" -\", out_vehiculos)\n",
    "print(\" -\", out_victimas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36e0436e-59b3-46ba-83dc-ad82e233d5cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "##### Cargar los CSV con Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d27a562-fde4-4729-b7b4-a2dad9d84ec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === RUTA en Volume UC (NO /Workspace) ===\n",
    "DATA_DIR = \"/Volumes/workspace/default/lab8/Data\"\n",
    "\n",
    "# Hechos\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW hechos_long AS\n",
    "SELECT\n",
    "  CAST(anio AS INT)                           AS anio,\n",
    "  dim1, dim1_val, dim2, dim2_val,\n",
    "  CAST(regexp_replace(valor, '[^0-9-]', '') AS INT) AS valor,\n",
    "  lower(grupo) AS grupo,\n",
    "  cuadro, csv\n",
    "FROM read_files('{DATA_DIR}/hechos_long.csv',\n",
    "                format => 'csv', header => true)\n",
    "WHERE lower(coalesce(dim1_val,'')) <> 'total'\n",
    "  AND lower(coalesce(dim2_val,'')) <> 'total'\n",
    "\"\"\")\n",
    "\n",
    "# Vehículos\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW vehiculos_long AS\n",
    "SELECT\n",
    "  CAST(anio AS INT)                           AS anio,\n",
    "  dim1, dim1_val, dim2, dim2_val,\n",
    "  CAST(regexp_replace(valor, '[^0-9-]', '') AS INT) AS valor,\n",
    "  lower(grupo) AS grupo,\n",
    "  cuadro, csv\n",
    "FROM read_files('{DATA_DIR}/vehiculos_long.csv',\n",
    "                format => 'csv', header => true)\n",
    "WHERE lower(coalesce(dim1_val,'')) <> 'total'\n",
    "  AND lower(coalesce(dim2_val,'')) <> 'total'\n",
    "\"\"\")\n",
    "\n",
    "# Víctimas\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW victimas_long AS\n",
    "SELECT\n",
    "  CAST(anio AS INT)                           AS anio,\n",
    "  dim1, dim1_val, dim2, dim2_val,\n",
    "  CAST(regexp_replace(valor, '[^0-9-]', '') AS INT) AS valor,\n",
    "  lower(grupo) AS grupo,\n",
    "  cuadro, csv\n",
    "FROM read_files('{DATA_DIR}/victimas_long.csv',\n",
    "                format => 'csv', header => true)\n",
    "WHERE lower(coalesce(dim1_val,'')) <> 'total'\n",
    "  AND lower(coalesce(dim2_val,'')) <> 'total'\n",
    "\"\"\")\n",
    "\n",
    "# --- Chequeos rápidos ---\n",
    "display(spark.sql(\"\"\"\n",
    "SELECT 'hechos' AS tabla, COUNT(*) AS filas FROM hechos_long\n",
    "UNION ALL\n",
    "SELECT 'vehiculos', COUNT(*) FROM vehiculos_long\n",
    "UNION ALL\n",
    "SELECT 'victimas', COUNT(*) FROM victimas_long\n",
    "\"\"\"))\n",
    "\n",
    "display(spark.sql(\"SELECT DISTINCT anio FROM hechos_long ORDER BY anio\"))\n",
    "display(spark.sql(\"SELECT DISTINCT anio FROM vehiculos_long ORDER BY anio\"))\n",
    "display(spark.sql(\"SELECT DISTINCT anio FROM victimas_long ORDER BY anio\"))\n",
    "\n",
    "# Ejemplos\n",
    "display(spark.sql(\"SELECT * FROM hechos_long    ORDER BY anio, cuadro LIMIT 15\"))\n",
    "display(spark.sql(\"SELECT * FROM vehiculos_long ORDER BY anio, cuadro LIMIT 15\"))\n",
    "display(spark.sql(\"SELECT * FROM victimas_long  ORDER BY anio, cuadro LIMIT 15\"))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "laboratorio8",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
