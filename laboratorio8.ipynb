{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8086f3be-c8f0-4720-8969-149fa54e9143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Laboratorio 8 - Transformaciones con Spark\n",
    "\n",
    "#### Edwin Ortega 22305\n",
    "#### Esteban Zambrano 22119\n",
    "#### Diego García 22404"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52c12913-162e-413d-b52f-4c24f27629a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Carga de Datos y Análisis Exploratorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8122b58-bde9-4955-90c4-f769567bb706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "##### Instalación a tener en cuenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a87e085-0922-4fb9-a808-b3210d6b6c40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85368396-49dd-4df4-8243-793560a99858",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Convertir Excel a un CSV por hoja\n",
    "##### Rutas, imports y utilidades "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "088d8e6a-1443-493f-ad28-29ed8a6a8d2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Se debe mover los excel en la carpeta 'Data' a un volumen de databricks. Se deben cambiar las rutas a las suyas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bd0a2b4-8a49-4876-bb09-143f99914857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Cambiar esto a su volumen\n",
    "BASE_DIR = \"/Volumes/workspace/default/lab8/\"\n",
    "\n",
    "SUBFOLDERS = [\"fallecidos-lesionados\", \"hechos\", \"vehiculos\"]\n",
    "OUTPUT_ROOT = os.path.join(BASE_DIR, \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "323dd6c1-c628-44db-9473-580f123d4c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "for sub in SUBFOLDERS:\n",
    "    os.makedirs(os.path.join(OUTPUT_ROOT, sub), exist_ok=True)\n",
    "\n",
    "print(\"Base:\", BASE_DIR)\n",
    "print(\"Salida:\", OUTPUT_ROOT)\n",
    "print(\"Subcarpetas:\", SUBFOLDERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c2561ea-5bfc-48a2-8a42-ed2c402fc4f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "manifest, errors = [], []\n",
    "\n",
    "for sub in SUBFOLDERS:\n",
    "    in_dir = Path(BASE_DIR) / sub\n",
    "    out_dir = Path(OUTPUT_ROOT) / sub\n",
    "\n",
    "    for f in sorted(in_dir.iterdir()):\n",
    "        if not f.is_file():\n",
    "            continue\n",
    "        if f.suffix.lower() not in {\".xlsx\", \".xls\"}:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Lee SOLO la primera hoja\n",
    "            df = pd.read_excel(f, sheet_name=0)\n",
    "            # Nombre base del archivo + .csv\n",
    "            csv_name = f.with_suffix(\".csv\").name\n",
    "            csv_path = out_dir / csv_name\n",
    "\n",
    "            # Guarda CSV sin índice\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            manifest.append((sub, str(f), str(csv_path)))\n",
    "        except Exception as e:\n",
    "            errors.append((str(f), repr(e)))\n",
    "\n",
    "print(f\"CSV generados: {len(manifest)}\")\n",
    "if errors:\n",
    "    print(f\"Archivos con error: {len(errors)} (muestra 5)\")\n",
    "    for p, err in errors[:5]:\n",
    "        print(\"-\", p, \"->\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f0b96fc-3146-4991-824a-8aed9a555ef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f76056f4-1903-4aac-8c72-b7961e2d7685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "INPUT_ROOT  = Path(BASE_DIR) / \"csv\"\n",
    "\n",
    "# Recolecta rutas de todos los CSV\n",
    "csv_files = []\n",
    "for sub in SUBFOLDERS:\n",
    "    folder = INPUT_ROOT / sub\n",
    "    if not folder.exists():\n",
    "        print(f\"⚠️ No existe: {folder}\")\n",
    "        continue\n",
    "    for f in sorted(folder.iterdir()):\n",
    "        if f.is_file() and f.suffix.lower() == \".csv\":\n",
    "            csv_files.append((sub, f))\n",
    "\n",
    "print(f\"Archivos CSV encontrados: {len(csv_files)}\")\n",
    "for i, (sub, f) in enumerate(csv_files[:10], start=1):\n",
    "    print(f\"{i:02d}. [{sub}] {f.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d466f89f-047c-43bf-bae2-5de0ab8ba0c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estandarización de datos\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def strip_accents_lower(text: str) -> str:\n",
    "    \"\"\"Convierte a minúsculas, elimina acentos/diéresis y comprime espacios.\"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    s = str(text).lower().strip()\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = s.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def normalize_headers(cols):\n",
    "    \"\"\"Normaliza encabezados: minus, sin acentos, espacios->_, solo [a-z0-9_].\"\"\"\n",
    "    norm = []\n",
    "    for c in cols:\n",
    "        s = strip_accents_lower(c)\n",
    "        s = re.sub(r\"[^a-z0-9_ ]\", \"\", s)\n",
    "        s = s.replace(\" \", \"_\")\n",
    "        s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "        norm.append(s or \"col\")\n",
    "    return norm\n",
    "\n",
    "def normalize_dataframe_text(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Devuelve una copia con encabezados normalizados y columnas object en minus/sin acentos.\"\"\"\n",
    "    out = df.copy()\n",
    "    out.columns = normalize_headers(out.columns)\n",
    "    obj_cols = out.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    for c in obj_cols:\n",
    "        out[c] = out[c].map(strip_accents_lower)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d76dcbcc-b7ce-4abf-bf85-7b0288a1babc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_data   = {}\n",
    "clean_data = {}\n",
    "\n",
    "errors = []\n",
    "\n",
    "for sub, f in csv_files:\n",
    "    try:\n",
    "        # Detecta separador automáticamente; cae a coma si falla\n",
    "        try:\n",
    "            df = pd.read_csv(f, sep=None, engine=\"python\", encoding=\"utf-8\", encoding_errors=\"ignore\")\n",
    "        except Exception:\n",
    "            df = pd.read_csv(f, encoding=\"utf-8\", encoding_errors=\"ignore\")\n",
    "\n",
    "        df_clean = normalize_dataframe_text(df)\n",
    "\n",
    "        key = (sub, f.name)\n",
    "        raw_data[key] = df\n",
    "        clean_data[key] = df_clean\n",
    "\n",
    "    except Exception as e:\n",
    "        errors.append((str(f), repr(e)))\n",
    "\n",
    "print(f\"DataFrames cargados: {len(raw_data)} | Normalizados: {len(clean_data)}\")\n",
    "if errors:\n",
    "    print(f\"⚠️ Errores en {len(errors)} archivos (muestra 5):\")\n",
    "    for p, err in errors[:5]:\n",
    "        print(\"-\", p, \"->\", err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0405f5d-7a66-4ce7-bf55-e5f1dd7358fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Selección de columanas para Fallecidos/lesionados\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# columnas requeridas\n",
    "req_fall = [\"ano_ocu\",\"mes_ocu\",\"depto_ocu\",\"zona_ocu\",\"edad_per\",\"tipo_eve\",\"fall_les\"]\n",
    "\n",
    "sel_fallecidos = {}\n",
    "faltantes_fallecidos = []\n",
    "\n",
    "for (sub, fname), df in clean_data.items():\n",
    "    if sub != \"fallecidos-lesionados\":\n",
    "        continue\n",
    "\n",
    "    # detecta año desde el nombre del archivo (primer 4 dígitos)\n",
    "    m = re.search(r\"(\\d{4})\", fname)\n",
    "    anio = int(m.group(1)) if m else None\n",
    "\n",
    "    # crea columnas faltantes con \"ignorado\"\n",
    "    missing = [c for c in req_fall if c not in df.columns]\n",
    "    for c in missing:\n",
    "        df[c] = \"ignorado\"\n",
    "\n",
    "    # registra faltantes (uno por columna faltante)\n",
    "    for c in missing:\n",
    "        faltantes_fallecidos.append({\"anio\": anio, \"archivo\": fname, \"col_faltante\": c})\n",
    "\n",
    "    # deja solo las columnas requeridas en el orden pedido\n",
    "    sel = df[req_fall].copy()\n",
    "    sel_fallecidos[(sub, fname)] = sel\n",
    "\n",
    "# reporte\n",
    "reporte_fallecidos = pd.DataFrame(faltantes_fallecidos)\n",
    "display(reporte_fallecidos.sort_values([\"anio\",\"col_faltante\"]) if not reporte_fallecidos.empty \n",
    "        else pd.DataFrame(columns=[\"anio\",\"archivo\",\"col_faltante\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1608b460-4443-4e65-8fb0-2774f3ade811",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Selección de columanas para hechos\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "req_hechos = [\"ano_ocu\",\"hora_ocu\",\"mes_ocu\",\"dia_sem_ocu\",\"depto_ocu\",\"tipo_eve\"]\n",
    "\n",
    "sel_hechos = {}\n",
    "faltantes_hechos = []\n",
    "\n",
    "for (sub, fname), df in clean_data.items():\n",
    "    if sub != \"hechos\":\n",
    "        continue\n",
    "\n",
    "    m = re.search(r\"(\\d{4})\", fname)\n",
    "    anio = int(m.group(1)) if m else None\n",
    "\n",
    "    missing = [c for c in req_hechos if c not in df.columns]\n",
    "    for c in missing:\n",
    "        df[c] = \"ignorado\"\n",
    "\n",
    "    for c in missing:\n",
    "        faltantes_hechos.append({\"anio\": anio, \"archivo\": fname, \"col_faltante\": c})\n",
    "\n",
    "    sel = df[req_hechos].copy()\n",
    "    sel_hechos[(sub, fname)] = sel\n",
    "\n",
    "reporte_hechos = pd.DataFrame(faltantes_hechos)\n",
    "display(reporte_hechos.sort_values([\"anio\",\"col_faltante\"]) if not reporte_hechos.empty \n",
    "        else pd.DataFrame(columns=[\"anio\",\"archivo\",\"col_faltante\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62dce7a2-1460-4e4c-816c-a0ea1b98b2ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Selección de columanas para vehiculos\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "req_veh = [\"ano_ocu\",\"mes_ocu\",\"depto_ocu\",\"sexo_per\",\"tipo_veh\",\"marca_veh\",\"color_veh\",\"modelo_veh\",\"tipo_eve\"]\n",
    "\n",
    "sel_vehiculos = {}\n",
    "faltantes_vehiculos = []\n",
    "\n",
    "for (sub, fname), df in clean_data.items():\n",
    "    if sub != \"vehiculos\":\n",
    "        continue\n",
    "\n",
    "    m = re.search(r\"(\\d{4})\", fname)\n",
    "    anio = int(m.group(1)) if m else None\n",
    "\n",
    "    missing = [c for c in req_veh if c not in df.columns]\n",
    "    for c in missing:\n",
    "        df[c] = \"ignorado\"\n",
    "\n",
    "    for c in missing:\n",
    "        faltantes_vehiculos.append({\"anio\": anio, \"archivo\": fname, \"col_faltante\": c})\n",
    "\n",
    "    sel = df[req_veh].copy()\n",
    "    sel_vehiculos[(sub, fname)] = sel\n",
    "\n",
    "reporte_vehiculos = pd.DataFrame(faltantes_vehiculos)\n",
    "display(reporte_vehiculos.sort_values([\"anio\",\"col_faltante\"]) if not reporte_vehiculos.empty \n",
    "        else pd.DataFrame(columns=[\"anio\",\"archivo\",\"col_faltante\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74e995ad-6be9-4f5e-8951-99c75795d9a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "YEAR = 2019  # ← cambia el año\n",
    "\n",
    "keys_veh = [k for k in sel_vehiculos.keys() if re.search(fr\"\\b{YEAR}\\b\", k[1])]\n",
    "keys_veh = sorted(keys_veh, key=lambda x: x[1])\n",
    "\n",
    "if keys_veh:\n",
    "    sample_key = keys_veh[0]\n",
    "    print(\"Mostrando (vehiculos):\", sample_key)\n",
    "    display(sel_vehiculos[sample_key].head(10))\n",
    "else:\n",
    "    print(f\"No se encontró archivo de vehiculos con año {YEAR} en el nombre.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "207087fe-528d-4d0c-bee6-000a0ca2d407",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estandarización de datos generales\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "YEAR_MIN, YEAR_MAX = 2013, 2020\n",
    "\n",
    "# mapas\n",
    "MES_MAP = {\n",
    "    1:\"enero\", 2:\"febrero\", 3:\"marzo\", 4:\"abril\", 5:\"mayo\", 6:\"junio\",\n",
    "    7:\"julio\", 8:\"agosto\", 9:\"septiembre\", 10:\"octubre\", 11:\"noviembre\", 12:\"diciembre\"\n",
    "}\n",
    "\n",
    "DEPTO_MAP = {\n",
    "     1:\"guatemala\",  2:\"el progreso\", 3:\"sacatepequez\", 4:\"chimaltenango\",\n",
    "     5:\"escuintla\",  6:\"santa rosa\",  7:\"solola\",       8:\"totonicapan\",\n",
    "     9:\"quetzaltenango\", 10:\"suchitepequez\", 11:\"retalhuleu\", 12:\"san marcos\",\n",
    "    13:\"huehuetenango\", 14:\"quiche\", 15:\"baja verapaz\", 16:\"alta verapaz\",\n",
    "    17:\"peten\", 18:\"izabal\", 19:\"zacapa\", 20:\"chiquimula\", 21:\"jalapa\", 22:\"jutiapa\"\n",
    "}\n",
    "\n",
    "TIPO_EVE_MAP = {\n",
    "     1:\"colision\", 2:\"choque\", 3:\"vuelco\", 4:\"caida\", 5:\"atropello\",\n",
    "     6:\"perdida de control\", 7:\"colision contra animal\", 8:\"exceso de pasaje\",\n",
    "     9:\"asfalto mojado\", 10:\"exceso de velocidad\", 11:\"desperfectos mecanicos\",\n",
    "    12:\"incendio\", 99:\"ignorado\"\n",
    "}\n",
    "\n",
    "def _apply_general_changes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # 1) \"ignorada\" -> \"ignorado\" en todas las columnas de texto\n",
    "    obj_cols = out.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    for c in obj_cols:\n",
    "        out[c] = out[c].replace(\"ignorada\", \"ignorado\")\n",
    "\n",
    "    # 2) mes_ocu numérico -> nombre\n",
    "    if \"mes_ocu\" in out.columns:\n",
    "        _num = pd.to_numeric(out[\"mes_ocu\"], errors=\"coerce\")\n",
    "        mask = _num.notna()\n",
    "        out.loc[mask, \"mes_ocu\"] = _num[mask].astype(int).map(MES_MAP).fillna(out.loc[mask, \"mes_ocu\"])\n",
    "\n",
    "    # 3) depto_ocu numérico -> nombre\n",
    "    if \"depto_ocu\" in out.columns:\n",
    "        _num = pd.to_numeric(out[\"depto_ocu\"], errors=\"coerce\")\n",
    "        mask = _num.notna()\n",
    "        out.loc[mask, \"depto_ocu\"] = _num[mask].astype(int).map(DEPTO_MAP).fillna(out.loc[mask, \"depto_ocu\"])\n",
    "\n",
    "    # 4) tipo_eve numérico -> nombre\n",
    "    if \"tipo_eve\" in out.columns:\n",
    "        _num = pd.to_numeric(out[\"tipo_eve\"], errors=\"coerce\")\n",
    "        mask = _num.notna()\n",
    "        out.loc[mask, \"tipo_eve\"] = _num[mask].astype(int).map(TIPO_EVE_MAP).fillna(out.loc[mask, \"tipo_eve\"])\n",
    "\n",
    "    return out\n",
    "\n",
    "def _year_from_filename(fname: str):\n",
    "    m = re.search(r\"(\\d{4})\", fname)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "\n",
    "changed_fallecidos = {}\n",
    "changed_hechos = {}\n",
    "changed_vehiculos = {}\n",
    "\n",
    "for key, df in sel_fallecidos.items():\n",
    "    sub, fname = key\n",
    "    yr = _year_from_filename(fname)\n",
    "    if yr is not None and (YEAR_MIN <= yr <= YEAR_MAX):\n",
    "        changed_fallecidos[key] = _apply_general_changes(df)\n",
    "    else:\n",
    "        changed_fallecidos[key] = df\n",
    "\n",
    "for key, df in sel_hechos.items():\n",
    "    sub, fname = key\n",
    "    yr = _year_from_filename(fname)\n",
    "    if yr is not None and (YEAR_MIN <= yr <= YEAR_MAX):\n",
    "        changed_hechos[key] = _apply_general_changes(df)\n",
    "    else:\n",
    "        changed_hechos[key] = df\n",
    "\n",
    "for key, df in sel_vehiculos.items():\n",
    "    sub, fname = key\n",
    "    yr = _year_from_filename(fname)\n",
    "    if yr is not None and (YEAR_MIN <= yr <= YEAR_MAX):\n",
    "        changed_vehiculos[key] = _apply_general_changes(df)\n",
    "    else:\n",
    "        changed_vehiculos[key] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5e7a08a-1bd6-423d-a756-b3a75f606b87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estandarización de datos de fallecidos-lesionados\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "YEAR_MIN, YEAR_MAX = 2013, 2020\n",
    "\n",
    "def _year_from_filename(fname: str):\n",
    "    m = re.search(r\"(\\d{4})\", fname)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "final_fallecidos = {}\n",
    "\n",
    "for key, df in changed_fallecidos.items():\n",
    "    sub, fname = key\n",
    "    yr = _year_from_filename(fname)\n",
    "\n",
    "    if yr is None or not (YEAR_MIN <= yr <= YEAR_MAX):\n",
    "        # fuera de rango: lo dejamos igual\n",
    "        final_fallecidos[key] = df\n",
    "        continue\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    # zona_ocu: 99 -> \"ignorado\"\n",
    "    if \"zona_ocu\" in out.columns:\n",
    "        znum = pd.to_numeric(out[\"zona_ocu\"], errors=\"coerce\")\n",
    "        out.loc[znum == 99, \"zona_ocu\"] = \"ignorado\"\n",
    "\n",
    "    # edad_per: 999 -> \"ignorado\"\n",
    "    if \"edad_per\" in out.columns:\n",
    "        ednum = pd.to_numeric(out[\"edad_per\"], errors=\"coerce\")\n",
    "        out.loc[ednum == 999, \"edad_per\"] = \"ignorado\"\n",
    "\n",
    "    # fall_les: 1 -> \"fallecido\"; 2 -> \"lesionado\"\n",
    "    if \"fall_les\" in out.columns:\n",
    "        # Maneja valores numéricos o string\n",
    "        out[\"fall_les\"] = (\n",
    "            out[\"fall_les\"]\n",
    "            .astype(str).str.strip()\n",
    "            .replace({\"1\": \"fallecido\", \"2\": \"lesionado\"})\n",
    "        )\n",
    "\n",
    "    for c in [\"zona_ocu\", \"edad_per\", \"fall_les\", \"mes_ocu\",\"depto_ocu\", \"tipo_eve\"]:\n",
    "        if c in out.columns:\n",
    "            out[c] = out[c].astype(\"string\").fillna(\"ignorado\")\n",
    "\n",
    "    final_fallecidos[key] = out\n",
    "\n",
    "# Vista rápida\n",
    "if final_fallecidos:\n",
    "    sample_key = sorted(final_fallecidos.keys(), key=lambda k: k[1])[0]\n",
    "    print(\"Mostrando (fallecidos-lesionados):\", sample_key)\n",
    "    display(final_fallecidos[sample_key].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab2a4f9e-230e-410c-bd72-bd556c26ce2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estandarización de datos de hechos\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "YEAR_MIN, YEAR_MAX = 2013, 2020\n",
    "\n",
    "def _year_from_filename(fname: str):\n",
    "    m = re.search(r\"(\\d{4})\", fname)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "# 1=lunes, 2=martes, ..., 7=domingo (sin tildes)\n",
    "DOW_MAP = {1:\"lunes\", 2:\"martes\", 3:\"miercoles\", 4:\"jueves\", 5:\"viernes\", 6:\"sabado\", 7:\"domingo\"}\n",
    "\n",
    "final_hechos = {}\n",
    "\n",
    "for key, df in changed_hechos.items():\n",
    "    sub, fname = key\n",
    "    yr = _year_from_filename(fname)\n",
    "\n",
    "    if yr is None or not (YEAR_MIN <= yr <= YEAR_MAX):\n",
    "        final_hechos[key] = df\n",
    "        continue\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    if \"dia_sem_ocu\" in out.columns:\n",
    "        nums = pd.to_numeric(out[\"dia_sem_ocu\"], errors=\"coerce\")\n",
    "        mask = nums.notna()\n",
    "        out.loc[mask, \"dia_sem_ocu\"] = nums[mask].astype(int).map(DOW_MAP).fillna(out.loc[mask, \"dia_sem_ocu\"])\n",
    "        # Evita problemas Arrow: homogeniza a string y rellena nulos\n",
    "        out[\"dia_sem_ocu\"] = out[\"dia_sem_ocu\"].astype(\"string\").fillna(\"ignorado\")\n",
    "\n",
    "    final_hechos[key] = out\n",
    "\n",
    "# Vista rápida\n",
    "if final_hechos:\n",
    "    sample_key = sorted(final_hechos.keys(), key=lambda k: k[1])[1]\n",
    "    print(\"Mostrando (hechos):\", sample_key)\n",
    "    display(final_hechos[sample_key].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d95305ed-b921-4495-823c-cbe57e411d93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estandarización de datos de vehiculos\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Rangos\n",
    "YEAR_MIN_GENERAL, YEAR_MAX_GENERAL = 2013, 2020\n",
    "YEAR_MIN_DROP_MARCA, YEAR_MAX_DROP_MARCA = 2013, 2023\n",
    "\n",
    "def _year_from_filename(fname: str):\n",
    "    m = re.search(r\"(\\d{4})\", fname)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "# Mapas (texto ya sin acentos y en minusculas)\n",
    "SEXO_MAP = {1: \"hombre\", 2: \"mujer\", 9: \"ignorado\"}\n",
    "\n",
    "TIPO_VEH_MAP = {\n",
    "     1: \"automovil\",  2: \"camioneta\",  3: \"pick_up\",      4: \"motocicleta\",\n",
    "     5: \"camion\",     6: \"cabezal\",    7: \"bus_extraurbano\", 8: \"jeep\",\n",
    "     9: \"microbus\",  10: \"taxi\",      11: \"panel\",       12: \"bus_urbano\",\n",
    "    13: \"tractor\",   14: \"moto_taxi\", 15: \"furgon\",      16: \"grua\",\n",
    "    17: \"bus_escolar\", 18: \"bicicleta\", 99: \"ignorado\"\n",
    "}\n",
    "\n",
    "COLOR_VEH_MAP = {\n",
    "     1: \"rojo\",       2: \"blanco\",     3: \"azul\",        4: \"gris\",\n",
    "     5: \"negro\",      6: \"verde\",      7: \"amarillo\",    8: \"celeste\",\n",
    "     9: \"corinto\",   10: \"cafe\",      11: \"beige\",      12: \"turquesa\",\n",
    "    13: \"marfil\",    14: \"anaranjado\", 15: \"aqua\",       16: \"morado\",\n",
    "    17: \"rosado\",    99: \"ignorado\"\n",
    "}\n",
    "\n",
    "final_vehiculos = {}\n",
    "\n",
    "for key, df in changed_vehiculos.items():\n",
    "    sub, fname = key\n",
    "    yr = _year_from_filename(fname)\n",
    "    out = df.copy()\n",
    "\n",
    "    # ---- Reglas generales para 2013–2020 ----\n",
    "    if yr is not None and (YEAR_MIN_GENERAL <= yr <= YEAR_MAX_GENERAL):\n",
    "        # sexo_per\n",
    "        if \"sexo_per\" in out.columns:\n",
    "            _num = pd.to_numeric(out[\"sexo_per\"], errors=\"coerce\")\n",
    "            mask = _num.notna()\n",
    "            out.loc[mask, \"sexo_per\"] = _num[mask].astype(int).map(SEXO_MAP).fillna(out.loc[mask, \"sexo_per\"])\n",
    "\n",
    "        # tipo_veh\n",
    "        if \"tipo_veh\" in out.columns:\n",
    "            _num = pd.to_numeric(out[\"tipo_veh\"], errors=\"coerce\")\n",
    "            mask = _num.notna()\n",
    "            out.loc[mask, \"tipo_veh\"] = _num[mask].astype(int).map(TIPO_VEH_MAP).fillna(out.loc[mask, \"tipo_veh\"])\n",
    "\n",
    "        # color_veh\n",
    "        if \"color_veh\" in out.columns:\n",
    "            _num = pd.to_numeric(out[\"color_veh\"], errors=\"coerce\")\n",
    "            mask = _num.notna()\n",
    "            out.loc[mask, \"color_veh\"] = _num[mask].astype(int).map(COLOR_VEH_MAP).fillna(out.loc[mask, \"color_veh\"])\n",
    "\n",
    "    # ---- Eliminar marca_veh para 2013–2023 ----\n",
    "    if yr is not None and (YEAR_MIN_DROP_MARCA <= yr <= YEAR_MAX_DROP_MARCA):\n",
    "        if \"marca_veh\" in out.columns:\n",
    "            out = out.drop(columns=[\"marca_veh\"])\n",
    "        elif \"modelo_veh\" in out.columns:\n",
    "            out = out.drop(columns=[\"modelo_veh\"])\n",
    "\n",
    "    # Homogeneizar tipos a string para evitar errores Arrow\n",
    "    for c in [\"sexo_per\", \"tipo_veh\", \"color_veh\"]:\n",
    "        if c in out.columns:\n",
    "            out[c] = out[c].astype(\"string\").fillna(\"ignorado\")\n",
    "\n",
    "    final_vehiculos[key] = out\n",
    "\n",
    "# Vista rápida\n",
    "if final_vehiculos:\n",
    "    sample_key = sorted(final_vehiculos.keys(), key=lambda k: k[1])[1]\n",
    "    print(\"Mostrando (vehiculos):\", sample_key)\n",
    "    display(final_vehiculos[sample_key].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c87c4628-cc2c-45f6-81bf-7a3a9a9db80b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sobreescribir fallecidos/lesionados\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = \"/Volumes/workspace/default/lab8\"\n",
    "OUT_ROOT = Path(BASE_DIR) / \"csv\"\n",
    "\n",
    "saved = 0\n",
    "for (sub, fname), df in final_fallecidos.items():\n",
    "    out_path = OUT_ROOT / sub / fname\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "    saved += 1\n",
    "\n",
    "print(f\"CSV sobrescritos (fallecidos-lesionados): {saved}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63efef1b-06a6-437d-a4d3-5113dbdb9d02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sobreescribir hechos\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = \"/Volumes/workspace/default/lab8\"\n",
    "OUT_ROOT = Path(BASE_DIR) / \"csv\"\n",
    "\n",
    "saved = 0\n",
    "for (sub, fname), df in final_hechos.items():\n",
    "    out_path = OUT_ROOT / sub / fname\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "    saved += 1\n",
    "\n",
    "print(f\"CSV sobrescritos (hechos): {saved}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb90c21a-5702-41f3-a064-998aea251fd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sobreescribir vehiculos\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = \"/Volumes/workspace/default/lab8\"\n",
    "OUT_ROOT = Path(BASE_DIR) / \"csv\"\n",
    "\n",
    "saved = 0\n",
    "for (sub, fname), df in final_vehiculos.items():\n",
    "    out_path = OUT_ROOT / sub / fname\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "    saved += 1\n",
    "\n",
    "print(f\"CSV sobrescritos (vehiculos): {saved}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "285a8d90-b636-41cb-8070-7308fd7891bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85b156e3-1def-4705-b7cc-eefb6065632f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================== SETUP CSV → SPARK ==================\n",
    "# Lee todos los CSV desde tu Unity Catalog Volume sin usar \"file:\" ni /FileStore.\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Lee TODOS los CSV por carpeta (varios años) con wildcard\n",
    "HECHOS_GLOB    = \"/Volumes/workspace/default/lab8/csv/hechos/*.csv\"\n",
    "VEHICULOS_GLOB = \"/Volumes/workspace/default/lab8/csv/vehiculos/*.csv\"\n",
    "\n",
    "hechos    = spark.read.csv(HECHOS_GLOB, header=True, inferSchema=True)\n",
    "vehiculos = spark.read.csv(VEHICULOS_GLOB, header=True, inferSchema=True)\n",
    "\n",
    "# Normaliza nombres canónicos que usan los incisos\n",
    "def latin_simplify(s: str) -> str:\n",
    "    return (s.strip().lower()\n",
    "            .replace(\"á\",\"a\").replace(\"é\",\"e\").replace(\"í\",\"i\").replace(\"ó\",\"o\").replace(\"ú\",\"u\")\n",
    "            .replace(\"ñ\",\"n\").replace(\" \",\"_\"))\n",
    "\n",
    "canon_map = {\n",
    "    \"anio\": {\"anio\",\"ano\",\"year\"},\n",
    "    \"mes\": {\"mes\",\"month\"},\n",
    "    \"departamento\": {\"departamento\",\"depto\",\"dpto\"},\n",
    "    \"municipio\": {\"municipio\"},\n",
    "    \"tipo_accidente\": {\"tipo_accidente\",\"tipo_de_accidente\",\"accidente_tipo\",\"clase_accidente\",\"tipo\"},\n",
    "    \"dia_semana\": {\"dia_semana\",\"dia_de_semana\",\"weekday\",\"day_of_week\"},\n",
    "    \"hora\": {\"hora\",\"hour\"},\n",
    "    \"valor\": {\"valor\",\"cantidad\",\"conteo\",\"n\",\"total\"},\n",
    "    \"color_vehiculo\": {\"color_vehiculo\",\"color\",\"vehiculo_color\"},\n",
    "    \"sexo_conductor\": {\"sexo_conductor\",\"sexo\",\"genero\"},\n",
    "}\n",
    "\n",
    "def rename_like(df):\n",
    "    cols = df.columns\n",
    "    norm = {c: latin_simplify(c) for c in cols}\n",
    "    inverse = {}\n",
    "    for canon, options in canon_map.items():\n",
    "        for c, n in norm.items():\n",
    "            if n in options and canon not in inverse:\n",
    "                inverse[canon] = c\n",
    "    out = df\n",
    "    for canon, orig in inverse.items():\n",
    "        if orig and latin_simplify(orig) != canon:\n",
    "            out = out.withColumnRenamed(orig, canon)\n",
    "    print(\"↪︎ Mapeo a canónicas:\", {k: inverse.get(k) for k in canon_map})\n",
    "    return out\n",
    "\n",
    "hechos    = rename_like(hechos)\n",
    "vehiculos = rename_like(vehiculos)\n",
    "\n",
    "# Limpieza mínima: mayúsculas y tipos\n",
    "for col in [\"departamento\",\"municipio\",\"tipo_accidente\",\"dia_semana\",\"color_vehiculo\",\"sexo_conductor\"]:\n",
    "    if col in hechos.columns:    hechos = hechos.withColumn(col, F.upper(F.trim(F.col(col))))\n",
    "    if col in vehiculos.columns: vehiculos = vehiculos.withColumn(col, F.upper(F.trim(F.col(col))))\n",
    "\n",
    "if \"hora\" in hechos.columns: hechos = hechos.withColumn(\"hora\", F.col(\"hora\").cast(\"int\"))\n",
    "if \"anio\" in hechos.columns: hechos = hechos.withColumn(\"anio\", F.col(\"anio\").cast(\"int\"))\n",
    "if \"anio\" in vehiculos.columns: vehiculos = vehiculos.withColumn(\"anio\", F.col(\"anio\").cast(\"int\"))\n",
    "\n",
    "print(\"Listo: 'hechos' y 'vehiculos' cargados desde /Volumes (sin 'file:').\")\n",
    "display(hechos.limit(5))\n",
    "display(vehiculos.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6f4f785-af0d-42ab-ae4f-df7c07615d01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Diagnóstico rápido de columnas y renombrado \"por contains\" ===\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def show_cols(df, name):\n",
    "    print(f\"\\n[{name}] columnas ({len(df.columns)}):\")\n",
    "    print(\", \".join(df.columns))\n",
    "\n",
    "show_cols(hechos, \"hechos\")\n",
    "show_cols(vehiculos, \"vehiculos\")\n",
    "\n",
    "# Mapeo flexible: busca substrings típicos en nombres de columna (ya sin acentos/espacios)\n",
    "import unicodedata, re\n",
    "\n",
    "def norm(s):\n",
    "    s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "    s = s.lower().strip().replace(\" \", \"_\")\n",
    "    s = re.sub(r\"[^a-z0-9_]\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "# patrones (regex \"contains\") para cada canónica\n",
    "PAT = {\n",
    "    \"anio\":          r\"(anio|a[o|ñ]o|year)\\b\",\n",
    "    \"mes\":           r\"\\bmes\\b|month|mes_num|mesnumero|mes_n|mes_?\\d\",\n",
    "    \"departamento\":  r\"depto|dpto|departament\",\n",
    "    \"municipio\":     r\"municip\",\n",
    "    \"tipo_accidente\":r\"tipo.*acciden|clase.*acciden|accidente.*tipo\",\n",
    "    \"dia_semana\":    r\"dia.*semana|weekday|day_?of_?week\",\n",
    "    \"hora\":          r\"^hora$|hour\\b|hora_?\\d\",\n",
    "    \"valor\":         r\"valor|cantidad|conteo|total|n(_|$)\",\n",
    "    \"color_vehiculo\":r\"color.*vehic|vehic.*color|\\bcolor\\b\",\n",
    "    \"sexo_conductor\":r\"(sexo|genero).*conduc|conduc.*(sexo|genero)|^sexo$|^genero$\",\n",
    "}\n",
    "\n",
    "def smart_rename(df):\n",
    "    cols = df.columns\n",
    "    nmap = {}   # canónica -> original\n",
    "    ncols = {c: norm(c) for c in cols}\n",
    "    for canon, rx in PAT.items():\n",
    "        rx_comp = re.compile(rx)\n",
    "        hit = next((c for c in cols if rx_comp.search(ncols[c]) or rx_comp.search(c.lower())), None)\n",
    "        if hit:\n",
    "            nmap[canon] = hit\n",
    "\n",
    "    # Aplica renombres solo si el nombre original difiere\n",
    "    out = df\n",
    "    for canon, orig in nmap.items():\n",
    "        if orig != canon:\n",
    "            out = out.withColumnRenamed(orig, canon)\n",
    "    print(\"Mapeo flexible:\", nmap)\n",
    "    return out\n",
    "\n",
    "hechos    = smart_rename(hechos)\n",
    "vehiculos = smart_rename(vehiculos)\n",
    "\n",
    "# Normalizaciones útiles\n",
    "for col in [\"departamento\",\"municipio\",\"tipo_accidente\",\"dia_semana\",\"color_vehiculo\",\"sexo_conductor\"]:\n",
    "    if col in hechos.columns:    hechos = hechos.withColumn(col, F.upper(F.trim(F.col(col))))\n",
    "    if col in vehiculos.columns: vehiculos = vehiculos.withColumn(col, F.upper(F.trim(F.col(col))))\n",
    "\n",
    "# Tipos y derivaciones\n",
    "if \"hora\" in hechos.columns: hechos = hechos.withColumn(\"hora\", F.col(\"hora\").cast(\"int\"))\n",
    "if \"anio\" in hechos.columns: hechos = hechos.withColumn(\"anio\", F.col(\"anio\").cast(\"int\"))\n",
    "if \"anio\" in vehiculos.columns: vehiculos = vehiculos.withColumn(\"anio\", F.col(\"anio\").cast(\"int\"))\n",
    "\n",
    "# Si no hay 'anio' o 'mes' pero sí hay 'fecha', derivarlos\n",
    "for df_name, df in [(\"hechos\", hechos), (\"vehiculos\", vehiculos)]:\n",
    "    cols = df.columns\n",
    "    fecha_like = next((c for c in cols if norm(c) in {\"fecha\",\"fecha_accidente\",\"fec_hecho\",\"f_accidente\"}), None)\n",
    "    if fecha_like:\n",
    "        ts = F.to_timestamp(F.col(fecha_like))\n",
    "        if \"anio\" not in cols:\n",
    "            df = df.withColumn(\"anio\", F.year(ts))\n",
    "        if \"mes\" not in cols:\n",
    "            df = df.withColumn(\"mes\", F.month(ts))\n",
    "        if df_name == \"hechos\":\n",
    "            hechos = df\n",
    "        else:\n",
    "            vehiculos = df\n",
    "\n",
    "print(\"\\nEsquemas tras normalización:\")\n",
    "hechos.printSchema()\n",
    "vehiculos.printSchema()\n",
    "\n",
    "display(hechos.limit(5))\n",
    "display(vehiculos.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72381b2e-0d75-4fc9-8257-26a48ed6f04c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ---------- Normalización mínima ----------\n",
    "hechos = (hechos\n",
    "    .withColumn(\"ano_ocu\", F.col(\"ano_ocu\").cast(\"int\"))\n",
    "    .withColumn(\"departamento\", F.upper(F.trim(F.col(\"departamento\"))))\n",
    "    .withColumn(\"tipo_eve\", F.upper(F.trim(F.col(\"tipo_eve\"))))\n",
    "    .withColumn(\"dia_sem_ocu\", F.upper(F.trim(F.col(\"dia_sem_ocu\"))))\n",
    "    .withColumn(\"hora_int\", F.regexp_extract(F.col(\"hora_ocu\"), r\"\\d+\", 0).cast(\"int\"))\n",
    ")\n",
    "\n",
    "vehiculos = (vehiculos\n",
    "    .withColumn(\"ano_ocu\", F.col(\"ano_ocu\").cast(\"int\"))\n",
    "    .withColumn(\"departamento\", F.upper(F.trim(F.col(\"departamento\"))))\n",
    "    .withColumn(\"tipo_eve\", F.upper(F.trim(F.col(\"tipo_eve\"))))\n",
    "    .withColumn(\"color_veh\", F.upper(F.trim(F.col(\"color_veh\"))))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79349e31-ca22-4fa4-af5a-4d83a2ce4b59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FALLE_GLOB = \"/Volumes/workspace/default/lab8/csv/fallecidos-lesionados/*.csv\"\n",
    "fallecidos_lesionados = spark.read.csv(FALLE_GLOB, header=True, inferSchema=True)\n",
    "\n",
    "print(\"Listo: 'fallecidos_lesionados' cargado.\")\n",
    "display(fallecidos_lesionados.limit(5))\n",
    "\n",
    "VEHICULOS_GLOB = \"/Volumes/workspace/default/lab8/csv/vehiculos/*.csv\"\n",
    "\n",
    "vehiculos_raw = (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", False)\n",
    "    .csv(VEHICULOS_GLOB)\n",
    ")\n",
    "\n",
    "vehiculos = (vehiculos_raw\n",
    "    .withColumn(\n",
    "        \"ano_ocu_int\",\n",
    "        F.expr(\"\"\"\n",
    "            try_cast(\n",
    "              nullif(regexp_replace(coalesce(ano_ocu,''), '[^0-9]', ''), '')\n",
    "              as int\n",
    "            )\n",
    "        \"\"\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"modelo_veh_digits\",\n",
    "        F.regexp_extract(F.coalesce(F.col(\"modelo_veh\"), F.lit(\"\")), r\"(\\d{4})\", 1)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"modelo_veh_int\",\n",
    "        F.when(\n",
    "            F.col(\"modelo_veh_digits\") != \"\",\n",
    "            F.expr(\"try_cast(modelo_veh_digits as int)\")\n",
    "        )\n",
    "        .otherwise(F.lit(None).cast(\"int\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"modelo_veh_int\",\n",
    "        F.when(F.col(\"modelo_veh_int\").between(1900, 2099), F.col(\"modelo_veh_int\"))\n",
    "         .otherwise(F.lit(None).cast(\"int\"))\n",
    "    )\n",
    "    .drop(\"modelo_veh_digits\")\n",
    ")\n",
    "\n",
    "print(\"Listo: 'vehiculos' cargado.\")\n",
    "display(vehiculos.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f807a73-8fa0-4764-b5d7-1129467f2cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Análisis exploratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c72277c4-d69b-4e4c-8541-d7fc597d5d1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================== 1.1) Conteo y muestreo ==================\n",
    "n_hechos     = hechos.count()\n",
    "n_vehiculos  = vehiculos.count()\n",
    "n_fall_les   = fallecidos_lesionados.count()\n",
    "\n",
    "print(f\"hechos: {n_hechos:,}\")\n",
    "print(f\"vehiculos: {n_vehiculos:,}\")\n",
    "print(f\"fallecidos_lesionados: {n_fall_les:,}\")\n",
    "\n",
    "print(\"\\n[hechos]\");            hechos.select(\"ano_ocu\",\"mes_ocu\",\"dia_sem_ocu\",\"departamento\",\"tipo_eve\",\"hora_ocu\").show(10, truncate=False)\n",
    "print(\"\\n[vehiculos]\");         vehiculos.select(\"ano_ocu\",\"mes_ocu\",\"sexo_per\",\"tipo_veh\",\"color_veh\",\"modelo_veh\",\"tipo_eve\").show(10, truncate=False)\n",
    "print(\"\\n[fallecidos_lesionados]\"); fallecidos_lesionados.select(\"ano_ocu\",\"mes_ocu\",\"depto_ocu\",\"zona_ocu\",\"edad_per\",\"tipo_eve\",\"fall_les\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f89ff9-0385-465c-898b-90479f6b6178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================== 1.2) Decribe y summary hechos ==================\n",
    "hechos_num = hechos.select(\n",
    "    F.expr(\"try_cast(ano_ocu as int)\").alias(\"ano_ocu_int\"),\n",
    "    F.expr(\"try_cast(regexp_extract(hora_ocu, '\\\\\\\\d+', 0) as int)\").alias(\"hora_ocu_int\")\n",
    ")\n",
    "\n",
    "print(\"[hechos] describe()\")\n",
    "display(hechos_num.describe())\n",
    "\n",
    "print(\"[hechos] summary()\")\n",
    "display(hechos_num.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba2e56c5-de73-4a32-bcb9-ac566189c2ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================== 1.3) Decribe y summary vehículos ==================\n",
    "vehiculos_num = vehiculos.select(\"ano_ocu_int\",\"modelo_veh_int\")\n",
    "\n",
    "print(\"[vehiculos] describe()\")\n",
    "display(vehiculos_num.describe())\n",
    "\n",
    "print(\"[vehiculos] summary()\")\n",
    "display(vehiculos_num.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d970210-a4ed-40ad-a7a2-0dd1b0d6f6ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================== 1.4) Decribe y summary fallecidos/lesionados ==================\n",
    "falle_ll_num = fallecidos_lesionados.select(\n",
    "    F.expr(\"try_cast(ano_ocu as int)\").alias(\"ano_ocu_int\"),\n",
    "    F.expr(\"\"\"\n",
    "        case when lower(coalesce(edad_per,'')) in ('ignorado','ignorada','')\n",
    "             then null\n",
    "             else try_cast(edad_per as int)\n",
    "        end\n",
    "    \"\"\").alias(\"edad_per_int\"),\n",
    "    F.expr(\"\"\"\n",
    "        case when lower(coalesce(zona_ocu,'')) in ('ignorado','ignorada','')\n",
    "             then null\n",
    "             else try_cast(zona_ocu as int)\n",
    "        end\n",
    "    \"\"\").alias(\"zona_ocu_int\")\n",
    ")\n",
    "\n",
    "print(\"[fallecidos_lesionados] describe()\")\n",
    "display(falle_ll_num.describe())\n",
    "\n",
    "print(\"[fallecidos_lesionados] summary()\")\n",
    "display(falle_ll_num.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8baf900c-0453-4026-91fe-88e1c2a3b5fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================== 2) Comparación de años ==================\n",
    "\n",
    "# Hechos: años\n",
    "hechos_years = (hechos\n",
    "    .select(F.expr(\"try_cast(ano_ocu as int)\").alias(\"anio\"))\n",
    "    .where(F.col(\"anio\").isNotNull())\n",
    "    .distinct().orderBy(\"anio\")\n",
    ")\n",
    "print(\"[hechos] años disponibles:\")\n",
    "hechos_years.show(200, truncate=False)\n",
    "\n",
    "# Vehículos: si ya tienes 'ano_ocu_int', úsalo; si no, intenta try_cast\n",
    "vehiculos_years_col = \"ano_ocu_int\" if \"ano_ocu_int\" in vehiculos.columns else \"anio\"\n",
    "vehiculos_years = (vehiculos\n",
    "    .select(F.col(vehiculos_years_col).alias(\"anio\") if \"ano_ocu_int\" in vehiculos.columns\n",
    "            else F.expr(\"try_cast(ano_ocu as int)\").alias(\"anio\"))\n",
    "    .where(F.col(\"anio\").isNotNull())\n",
    "    .distinct().orderBy(\"anio\")\n",
    ")\n",
    "print(\"[vehiculos] años disponibles:\")\n",
    "vehiculos_years.show(200, truncate=False)\n",
    "\n",
    "# Fallecidos/Lesionados: años\n",
    "falle_years = (fallecidos_lesionados\n",
    "    .select(F.expr(\"try_cast(ano_ocu as int)\").alias(\"anio\"))\n",
    "    .where(F.col(\"anio\").isNotNull())\n",
    "    .distinct().orderBy(\"anio\")\n",
    ")\n",
    "print(\"[fallecidos_lesionados] años disponibles:\")\n",
    "falle_years.show(200, truncate=False)\n",
    "\n",
    "# Comparar aparición\n",
    "all_years = (hechos_years.select(\"anio\")\n",
    "             .union(vehiculos_years.select(\"anio\"))\n",
    "             .union(falle_years.select(\"anio\"))\n",
    "             .distinct())\n",
    "\n",
    "tabla = (all_years\n",
    "    .join(hechos_years.withColumn(\"in_hechos\", F.lit(True)), [\"anio\"], \"left\")\n",
    "    .join(vehiculos_years.withColumn(\"in_vehiculos\", F.lit(True)), [\"anio\"], \"left\")\n",
    "    .join(falle_years.withColumn(\"in_fallecidos_lesionados\", F.lit(True)), [\"anio\"], \"left\")\n",
    "    .select(\n",
    "        \"anio\",\n",
    "        F.coalesce(\"in_hechos\", F.lit(False)).alias(\"hechos\"),\n",
    "        F.coalesce(\"in_vehiculos\", F.lit(False)).alias(\"vehiculos\"),\n",
    "        F.coalesce(\"in_fallecidos_lesionados\", F.lit(False)).alias(\"fallecidos_lesionados\")\n",
    "    )\n",
    "    .orderBy(\"anio\")\n",
    ")\n",
    "\n",
    "print(\"Tabla de presencia por año:\")\n",
    "tabla.show(200, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2e1c216-dda7-41c0-a9b1-7b287718b996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================== 3) Diferentes tipos de accidentes ==================\n",
    "\n",
    "# Hechos\n",
    "print(\"[hechos] tipos de accidentes:\")\n",
    "hechos.select(F.upper(F.trim(F.col(\"tipo_eve\"))).alias(\"tipo_accidente\")) \\\n",
    "     .distinct() \\\n",
    "     .orderBy(\"tipo_accidente\") \\\n",
    "     .show(100, truncate=False)\n",
    "\n",
    "# Vehículos\n",
    "print(\"[vehículos] tipos de accidentes:\")\n",
    "vehiculos.select(F.upper(F.trim(F.col(\"tipo_eve\"))).alias(\"tipo_accidente\")) \\\n",
    "        .distinct() \\\n",
    "        .orderBy(\"tipo_accidente\") \\\n",
    "        .show(100, truncate=False)\n",
    "\n",
    "\n",
    "# Fallecidos/lesionados\n",
    "print(\"[fallecidos/lesionados] tipos de accidentes:\")\n",
    "fallecidos_lesionados.select(F.upper(F.trim(F.col(\"tipo_eve\"))).alias(\"tipo_accidente\")) \\\n",
    "                     .distinct() \\\n",
    "                     .orderBy(\"tipo_accidente\") \\\n",
    "                     .show(100, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dadfd64f-70de-44e9-9308-70ca2cddb2e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================== 4) Departamentos únicos ==================\n",
    "\n",
    "# Hechos\n",
    "hechos.select(\"departamento\").distinct().orderBy(\"departamento\").show(50, truncate=False)\n",
    "\n",
    "# Vehículos\n",
    "vehiculos.select(\"depto_ocu\").distinct().orderBy(\"depto_ocu\").show(50, truncate=False)\n",
    "\n",
    "\n",
    "# Fallecidos/lesionados\n",
    "fallecidos_lesionados.select(\"depto_ocu\").distinct().orderBy(\"depto_ocu\").show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "500a4387-bb7f-45ef-94c7-6e42bdddb04c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Preguntas de análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30018711-d0e7-464a-90c1-58914e267c48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyA9PT09PT09PT09PT09PT09PT0gNSkgVG90YWwgZGUgYWNjaWRlbnRlcyBwb3IgYcOxbyB5IGRlcGFydGFtZW50byA9PT09PT09PT09PT09PT09PT0KYWNjX2FuaW9fZGVwdG8gPSAoaGVjaG9zCiAgICAuZ3JvdXBCeSgiYW5vX29jdSIsImRlcGFydGFtZW50byIpCiAgICAuYWdnKEYuY291bnQoRi5saXQoMSkpLmFsaWFzKCJhY2NpZGVudGVzIikpCiAgICAub3JkZXJCeSgiYW5vX29jdSIsImRlcGFydGFtZW50byIpCikKZGlzcGxheShhY2NfYW5pb19kZXB0byk=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewad167c2\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewad167c2\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewad167c2\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewad167c2) SELECT `ano_ocu`,SUM(`accidentes`) `column_36ff4f50477`,`departamento` FROM q GROUP BY `ano_ocu`,`departamento`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewad167c2\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "departamento",
             "id": "column_36ff4f50478"
            },
            "x": {
             "column": "ano_ocu",
             "id": "column_36ff4f50476"
            },
            "y": [
             {
              "column": "accidentes",
              "id": "column_36ff4f50477",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_36ff4f50477": {
             "name": "accidentes",
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "title": {
             "text": "Año y departamento"
            },
            "type": "-"
           },
           "yAxis": [
            {
             "title": {
              "text": "Accidentes"
             },
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "871eb0e5-a42a-4c6a-84e1-1c6d00ae6203",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 27.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "ano_ocu",
           "type": "column"
          },
          {
           "column": "departamento",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "ano_ocu",
           "type": "column"
          },
          {
           "alias": "column_36ff4f50477",
           "args": [
            {
             "column": "accidentes",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "departamento",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================== 5) Total de accidentes por año y departamento ==================\n",
    "acc_anio_depto = (hechos\n",
    "    .groupBy(\"ano_ocu\",\"departamento\")\n",
    "    .agg(F.count(F.lit(1)).alias(\"accidentes\"))\n",
    "    .orderBy(\"ano_ocu\",\"departamento\")\n",
    ")\n",
    "display(acc_anio_depto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6722d5e7-ebef-442a-96e8-7f13840d1f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Guatemala domina de forma consistente el conteo anual y muestra un crecimiento marcado hasta 2023, seguida por Escuintla y un grupo intermedio estable (Quetzaltenango, Santa Rosa, Sacatepéquez), mientras varios departamentos se mantienen bajos y relativamente planos. La serie revela una tendencia al alza 2013→2023 con una caída atípica en 2021, probablemente por subregistro o disrupciones operativas ligadas a la pandemia/cambios de captura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "096f6c92-693d-4846-a969-c3e4a0156972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyA9PT09PT09PT09PT09PT09PT0gNikgRMOtYSBkZSBsYSBzZW1hbmEgY29uIG3DoXMgYWNjaWRlbnRlcyBlbiAyMDI0ID09PT09PT09PT09PT09PT09PQphY2NfMjAyM19kaWEgPSAoaGVjaG9zCiAgICAuZmlsdGVyKEYuY29sKCJhbm9fb2N1Iik9PTIwMjMpCiAgICAuZ3JvdXBCeSgiZGlhX3NlbV9vY3UiKQogICAgLmFnZyhGLmNvdW50KEYubGl0KDEpKS5hbGlhcygiYWNjaWRlbnRlcyIpKQogICAgLm9yZGVyQnkoRi5kZXNjKCJhY2NpZGVudGVzIikpCikKZGlzcGxheShhY2NfMjAyM19kaWEp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewcc9d9f1\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewcc9d9f1\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewcc9d9f1\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewcc9d9f1) SELECT `dia_sem_ocu`,SUM(`accidentes`) `column_36ff4f50482` FROM q GROUP BY `dia_sem_ocu`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewcc9d9f1\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "dia_sem_ocu",
             "id": "column_36ff4f50481"
            },
            "y": [
             {
              "column": "accidentes",
              "id": "column_36ff4f50482",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_36ff4f50482": {
             "name": "accidentes",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "6a7ccd35-1d17-471b-b723-e2def03f4bf9",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 28.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "dia_sem_ocu",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "dia_sem_ocu",
           "type": "column"
          },
          {
           "alias": "column_36ff4f50482",
           "args": [
            {
             "column": "accidentes",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================== 6) Día de la semana con más accidentes en 2024 ==================\n",
    "acc_2023_dia = (hechos\n",
    "    .filter(F.col(\"ano_ocu\")==2023)\n",
    "    .groupBy(\"dia_sem_ocu\")\n",
    "    .agg(F.count(F.lit(1)).alias(\"accidentes\"))\n",
    "    .orderBy(F.desc(\"accidentes\"))\n",
    ")\n",
    "display(acc_2023_dia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69e0710f-ee64-4500-b8d7-2e138fb3efa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "En 2023 los fines de semana (domingo y sábado) concentran la mayor parte de los accidentes, lo que refleja un patrón de riesgo asociado a mayor movilidad. En contraste, los días laborales presentan cifras más bajas y relativamente estables, con miércoles como el de menor incidencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35535cad-518a-4de9-9da4-358d427700a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyA9PT09PT09PT09PT09PT09PT0gNykgRGlzdHJpYnVjacOzbiBwb3IgaG9yYSBlbiAiR3VhdGVtYWxhIiA9PT09PT09PT09PT09PT09PT0KZnJvbSBweXNwYXJrLnNxbCBpbXBvcnQgZnVuY3Rpb25zIGFzIEYKCiMgTGltcGlhciB5IGNvbnZlcnRpciBsYSBob3JhIGRlIGZvcm1hIHNlZ3VyYQpiYXNlX2d1YSA9IChoZWNob3MKICAgIC5maWx0ZXIoRi5jb2woImRlcGFydGFtZW50byIpPT0iR1VBVEVNQUxBIikKICAgIC53aXRoQ29sdW1uKAogICAgICAgICJob3JhIiwKICAgICAgICBGLmV4cHIoInRyeV9jYXN0KHJlZ2V4cF9leHRyYWN0KGhvcmFfb2N1LCAnXFxcXGQrJywgMCkgYXMgaW50KSIpCiAgICApCiAgICAuZmlsdGVyKChGLmNvbCgiaG9yYSIpLmlzTm90TnVsbCgpKSAmIChGLmNvbCgiaG9yYSIpPj0wKSAmIChGLmNvbCgiaG9yYSIpPD0yMykpCiAgICAuc2VsZWN0KCJob3JhIikKKQoKIyBNb3N0cmFyIGRpcmVjdGFtZW50ZTogZW4gUGxvdApkaXNwbGF5KGJhc2VfZ3VhKQoKYWNjX3Bvcl9ob3JhID0gKAogICAgYmFzZV9ndWEuZ3JvdXBCeSgiaG9yYSIpCiAgICAuYWdnKEYuY291bnQoIioiKS5hbGlhcygiYWNjaWRlbnRlcyIpKQogICAgLm9yZGVyQnkoImhvcmEiKQopCnRvdGFsID0gYWNjX3Bvcl9ob3JhLmFnZyhGLnN1bSgiYWNjaWRlbnRlcyIpKS5maXJzdCgpWzBdCmFjY19wb3JfaG9yYSA9IGFjY19wb3JfaG9yYS53aXRoQ29sdW1uKCJwb3JjZW50YWplIiwgRi5yb3VuZChGLmNvbCgiYWNjaWRlbnRlcyIpL0YubGl0KHRvdGFsKSoxMDAsIDIpKQoKZGlzcGxheShhY2NfcG9yX2hvcmEpCgojIFRvcCAzIHkgYm90dG9tIDMgcGFyYSBjb21lbnRhciBlbiBjb25jbHVzaW9uZXMKdG9wMyA9IGFjY19wb3JfaG9yYS5vcmRlckJ5KEYuZGVzYygiYWNjaWRlbnRlcyIpKS5saW1pdCgzKS5jb2xsZWN0KCkKYm90MyA9IGFjY19wb3JfaG9yYS5vcmRlckJ5KEYuYXNjKCJhY2NpZGVudGVzIikpLmxpbWl0KDMpLmNvbGxlY3QoKQoKZGVmIGZtdChyb3dzKToKICAgIHJldHVybiAiLCAiLmpvaW4oW2Yie3JbJ2hvcmEnXTowMmR9aCAoe3JbJ2FjY2lkZW50ZXMnXX0pIiBmb3IgciBpbiByb3dzXSkKCnByaW50KCJQaWNvcyAoVG9wIDMpOiIsIGZtdCh0b3AzKSkKcHJpbnQoIlZhbGxlcyAoQm90dG9tIDMpOiIsIGZtdChib3QzKSkK\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView46bece2\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView46bece2\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView46bece2\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView46bece2) ,min_max AS (SELECT `hora`,(SELECT MAX(`hora`) FROM q) `target_column_max`,(SELECT MIN(`hora`) FROM q) `target_column_min` FROM q) ,histogram_meta AS (SELECT `hora`,`target_column_min` `min_value`,IF(`target_column_max` = `target_column_min`,`target_column_max` + 1,`target_column_max`) `max_value`,(`target_column_max` - `target_column_min`) / 10 `step` FROM min_max) SELECT IF(ISNULL(`hora`),NULL,LEAST(WIDTH_BUCKET(`hora`,`min_value`,`max_value`,10),10)) `hora_BIN`,FIRST(`min_value` + ((IF(ISNULL(`hora`),NULL,LEAST(WIDTH_BUCKET(`hora`,`min_value`,`max_value`,10),10)) - 1) * `step`)) `hora_BIN_LOWER_BOUND`,FIRST(`step`) `hora_BIN_STEP`,COUNT(`hora`) `COUNT` FROM histogram_meta GROUP BY `hora_BIN`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView46bece2\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "hora",
             "id": "column_36ff4f50475"
            }
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "histogram",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numBins": 10,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {},
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "4f8e515f-3588-4a54-9ba3-e72d3aa805b1",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 30.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "hora_BIN",
           "type": "column"
          }
         ],
         "selects": [
          {
           "alias": "hora_BIN",
           "args": [
            {
             "column": "hora",
             "type": "column"
            },
            {
             "number": 10,
             "type": "number"
            }
           ],
           "function": "BIN",
           "type": "function"
          },
          {
           "alias": "hora_BIN_LOWER_BOUND",
           "args": [
            {
             "column": "hora",
             "type": "column"
            },
            {
             "number": 10,
             "type": "number"
            }
           ],
           "function": "BIN_LOWER_BOUND",
           "type": "function"
          },
          {
           "alias": "hora_BIN_STEP",
           "args": [
            {
             "column": "hora",
             "type": "column"
            },
            {
             "number": 10,
             "type": "number"
            }
           ],
           "function": "BIN_STEP",
           "type": "function"
          },
          {
           "alias": "COUNT",
           "args": [
            {
             "column": "hora",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================== 7) Distribución por hora en \"Guatemala\" ==================\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Limpiar y convertir la hora de forma segura\n",
    "base_gua = (hechos\n",
    "    .filter(F.col(\"departamento\")==\"GUATEMALA\")\n",
    "    .withColumn(\n",
    "        \"hora\",\n",
    "        F.expr(\"try_cast(regexp_extract(hora_ocu, '\\\\\\\\d+', 0) as int)\")\n",
    "    )\n",
    "    .filter((F.col(\"hora\").isNotNull()) & (F.col(\"hora\")>=0) & (F.col(\"hora\")<=23))\n",
    "    .select(\"hora\")\n",
    ")\n",
    "\n",
    "# Mostrar directamente: en Plot\n",
    "display(base_gua)\n",
    "\n",
    "acc_por_hora = (\n",
    "    base_gua.groupBy(\"hora\")\n",
    "    .agg(F.count(\"*\").alias(\"accidentes\"))\n",
    "    .orderBy(\"hora\")\n",
    ")\n",
    "total = acc_por_hora.agg(F.sum(\"accidentes\")).first()[0]\n",
    "acc_por_hora = acc_por_hora.withColumn(\"porcentaje\", F.round(F.col(\"accidentes\")/F.lit(total)*100, 2))\n",
    "\n",
    "display(acc_por_hora)\n",
    "\n",
    "# Top 3 y bottom 3 para comentar en conclusiones\n",
    "top3 = acc_por_hora.orderBy(F.desc(\"accidentes\")).limit(3).collect()\n",
    "bot3 = acc_por_hora.orderBy(F.asc(\"accidentes\")).limit(3).collect()\n",
    "\n",
    "def fmt(rows):\n",
    "    return \", \".join([f\"{r['hora']:02d}h ({r['accidentes']})\" for r in rows])\n",
    "\n",
    "print(\"Picos (Top 3):\", fmt(top3))\n",
    "print(\"Valles (Bottom 3):\", fmt(bot3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e4952d7-691f-48da-8aa1-fb79e2a3f08a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================== 8) Join Hechos ⨝ Vehículos (año, mes, depto, tipo) — FIX cast año ==================\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "HECHOS_GLOB    = \"/Volumes/workspace/default/lab8/csv/hechos/*.csv\"\n",
    "VEHICULOS_GLOB = \"/Volumes/workspace/default/lab8/csv/vehiculos/*.csv\"\n",
    "\n",
    "# Releer TODO como string\n",
    "h_src = spark.read.csv(HECHOS_GLOB, header=True, inferSchema=False)\n",
    "v_src = spark.read.csv(VEHICULOS_GLOB, header=True, inferSchema=False)\n",
    "\n",
    "# Unificar nombres mínimos\n",
    "def unify_cols(df):\n",
    "    ren = {}\n",
    "    if \"departamento\" not in df.columns and \"depto_ocu\" in df.columns:\n",
    "        ren[\"depto_ocu\"] = \"departamento\"\n",
    "    if \"anio\" not in df.columns:\n",
    "        if \"ano_ocu\" in df.columns: ren[\"ano_ocu\"] = \"anio_raw\"\n",
    "        elif \"year\"   in df.columns: ren[\"year\"]   = \"anio_raw\"\n",
    "    if \"mes\" not in df.columns:\n",
    "        if \"mes_ocu\" in df.columns: ren[\"mes_ocu\"] = \"mes_raw\"\n",
    "        elif \"month\"  in df.columns: ren[\"month\"]  = \"mes_raw\"\n",
    "    if \"tipo_accidente\" not in df.columns:\n",
    "        if \"tipo_eve\" in df.columns: ren[\"tipo_eve\"] = \"tipo_raw\"\n",
    "        elif \"tipo\"    in df.columns: ren[\"tipo\"]    = \"tipo_raw\"\n",
    "    out = df\n",
    "    for k,v in ren.items():\n",
    "        out = out.withColumnRenamed(k, v)\n",
    "    return out\n",
    "\n",
    "h = unify_cols(h_src)\n",
    "v = unify_cols(v_src)\n",
    "\n",
    "# ---------- Helpers (aceptan str o Column) ----------\n",
    "def _as_col(x):\n",
    "    return F.col(x) if isinstance(x, str) else x\n",
    "\n",
    "def clean_depto(x):\n",
    "    c = _as_col(x)\n",
    "    c = F.upper(F.trim(c))\n",
    "    c = F.translate(c, \"ÁÉÍÓÚáéíóúÑñ\", \"AEIOUaeiouNn\")\n",
    "    c = F.regexp_replace(c, r\"\\?\", \"E\")\n",
    "    c = F.regexp_replace(c, r\"\\s+\", \" \")\n",
    "    return c\n",
    "\n",
    "MESES = {\n",
    "    \"ENERO\":\"01\",\"FEBRERO\":\"02\",\"MARZO\":\"03\",\"ABRIL\":\"04\",\"MAYO\":\"05\",\"JUNIO\":\"06\",\n",
    "    \"JULIO\":\"07\",\"AGOSTO\":\"08\",\"SEPTIEMBRE\":\"09\",\"SETIEMBRE\":\"09\",\"OCTUBRE\":\"10\",\n",
    "    \"NOVIEMBRE\":\"11\",\"DICIEMBRE\":\"12\"\n",
    "}\n",
    "def clean_mes(x):\n",
    "    m = _as_col(x)\n",
    "    m = F.upper(F.trim(m))\n",
    "    m = F.translate(m, \"ÁÉÍÓÚáéíóú\", \"AEIOUaeiou\")\n",
    "    expr = None\n",
    "    for k,v in MESES.items():\n",
    "        expr = F.when(m == F.lit(k), F.lit(v)) if expr is None else expr.when(m == F.lit(k), F.lit(v))\n",
    "    num = F.lpad(F.regexp_extract(m, r\"\\d+\", 0), 2, \"0\")\n",
    "    expr = expr.otherwise(num) if expr is not None else num\n",
    "    return F.when(expr == \"\", None).otherwise(expr)\n",
    "\n",
    "def clean_anio(x):\n",
    "    c = _as_col(x)\n",
    "    extr = F.regexp_extract(c.cast(\"string\"), r\"\\d{4}\", 0)        # \"\" si no hay año\n",
    "    return F.when(F.length(extr) > 0, extr.cast(\"int\")).otherwise(F.lit(None).cast(\"int\"))\n",
    "\n",
    "def clean_tipo(x):\n",
    "    c = _as_col(x)\n",
    "    return F.upper(F.trim(c))\n",
    "\n",
    "def pick(df, *names):\n",
    "    for n in names:\n",
    "        if n in df.columns:\n",
    "            return F.col(n)\n",
    "    return F.lit(None)\n",
    "\n",
    "# Canónicas\n",
    "h = (h\n",
    "     .withColumn(\"departamento\",   clean_depto(\"departamento\"))\n",
    "     .withColumn(\"anio\",           clean_anio(pick(h, \"anio_raw\", \"anio\")))\n",
    "     .withColumn(\"mes\",            clean_mes(pick(h, \"mes_raw\",  \"mes\")))\n",
    "     .withColumn(\"tipo_accidente\", clean_tipo(pick(h, \"tipo_raw\",\"tipo_accidente\")))\n",
    "     .select(\"anio\",\"mes\",\"departamento\",\"tipo_accidente\")\n",
    ")\n",
    "v = (v\n",
    "     .withColumn(\"departamento\",   clean_depto(\"departamento\"))\n",
    "     .withColumn(\"anio\",           clean_anio(pick(v, \"anio_raw\", \"anio\")))\n",
    "     .withColumn(\"mes\",            clean_mes(pick(v, \"mes_raw\",  \"mes\")))\n",
    "     .withColumn(\"tipo_accidente\", clean_tipo(pick(v, \"tipo_raw\",\"tipo_accidente\")))\n",
    "     .select(\"anio\",\"mes\",\"departamento\",\"tipo_accidente\")\n",
    ")\n",
    "\n",
    "# Filtrar llaves válidas\n",
    "VALID_MES = r\"^(0[1-9]|1[0-2])$\"\n",
    "h_k = h.filter( (F.col(\"anio\").isNotNull()) & (F.col(\"mes\").rlike(VALID_MES)) & (F.col(\"departamento\").isNotNull()) & (F.col(\"tipo_accidente\").isNotNull()) )\n",
    "v_k = v.filter( (F.col(\"anio\").isNotNull()) & (F.col(\"mes\").rlike(VALID_MES)) & (F.col(\"departamento\").isNotNull()) & (F.col(\"tipo_accidente\").isNotNull()) )\n",
    "\n",
    "# Agregar y unir\n",
    "H  = h_k.groupBy(\"anio\",\"mes\",\"departamento\",\"tipo_accidente\").agg(F.count(F.lit(1)).alias(\"accidentes\"))\n",
    "V  = v_k.groupBy(\"anio\",\"mes\",\"departamento\",\"tipo_accidente\").agg(F.count(F.lit(1)).alias(\"vehiculos\"))\n",
    "HV = H.join(V, on=[\"anio\",\"mes\",\"departamento\",\"tipo_accidente\"], how=\"inner\")\n",
    "\n",
    "# Resultado\n",
    "hv_count = HV.count()\n",
    "print(f\"Registros combinados (H⨝V): {hv_count:,}\")\n",
    "\n",
    "# Vista + diagnósticos\n",
    "display(HV.orderBy(\"anio\",\"mes\",\"departamento\",\"tipo_accidente\").limit(20))\n",
    "print(\"Claves en Hechos   :\", H.count())\n",
    "print(\"Claves en Vehículos:\", V.count())\n",
    "print(\"Claves en el Join  :\", HV.count())\n",
    "\n",
    "no_en_V = H.join(V, on=[\"anio\",\"mes\",\"departamento\",\"tipo_accidente\"], how=\"left_anti\")\n",
    "no_en_H = V.join(H, on=[\"anio\",\"mes\",\"departamento\",\"tipo_accidente\"], how=\"left_anti\")\n",
    "print(\"Solo en Hechos (ejemplos):\");    display(no_en_V.limit(10))\n",
    "print(\"Solo en Vehículos (ejemplos):\"); display(no_en_H.limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58079db1-b60f-4bd7-a2e1-3bfc9564eebc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Con la llave compuesta (mes, año, departamento y tipo de accidente), conseguimos 8,694 registros combinados, lo que representa empatar aproximadamente el 96% de las 9,060 claves de vehículos y cerca del 86% de las 10,056 claves de hechos; en otras palabras, aunque el cruce es alto, no es absoluto. Los anti-joins revelan las razones de la falta de pares: diferencias en la codificación y las tildes en tipo_accidente, errores tipográficos en departamento y valores poco claros como IGNORADO y algunos años sin dígitos válidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07a8eae-8abe-4fd8-b3e8-906f71222e63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyA9PT09PT09PT09PT09PT09PT0gOSkgUHJvbWVkaW8gZGUgdmVow61jdWxvcyBwb3IgYWNjaWRlbnRlIHBvciBkZXBhcnRhbWVudG8gPT09PT09PT09PT09PT09PT09CmZyb20gcHlzcGFyay5zcWwgaW1wb3J0IGZ1bmN0aW9ucyBhcyBGCgojIFBhcnRpbW9zIGRlbCBqb2luIGRlbCBpbmNpc28gOAphc3NlcnQgJ0hWJyBpbiBnbG9iYWxzKCksICJObyBlbmN1ZW50cm8gJ0hWJy4gRWplY3V0YSBwcmltZXJvIGVsIGluY2lzbyA4LiIKaHZkZiA9IEhWCgojIEFncmVnYXIgcG9yIGRlcGFydGFtZW50bzogcHJvbWVkaW8gPSBzdW0odmVoaWN1bG9zKSAvIHN1bShhY2NpZGVudGVzKQpwcm9tX2RlcHRvID0gKAogICAgaHZkZi5ncm91cEJ5KCJkZXBhcnRhbWVudG8iKQogICAgICAgIC5hZ2coCiAgICAgICAgICAgIEYuc3VtKCJ2ZWhpY3Vsb3MiKS5hbGlhcygidmVoaWN1bG9zX3RvdCIpLAogICAgICAgICAgICBGLnN1bSgiYWNjaWRlbnRlcyIpLmFsaWFzKCJhY2NpZGVudGVzX3RvdCIpCiAgICAgICAgKQogICAgICAgIC53aXRoQ29sdW1uKAogICAgICAgICAgICAidmVoaWN1bG9zX3Bvcl9hY2NpZGVudGUiLAogICAgICAgICAgICBGLndoZW4oRi5jb2woImFjY2lkZW50ZXNfdG90IikgPiAwLCBGLmNvbCgidmVoaWN1bG9zX3RvdCIpL0YuY29sKCJhY2NpZGVudGVzX3RvdCIpKQogICAgICAgICAgICAgLm90aGVyd2lzZShGLmxpdChOb25lKSkKICAgICAgICApCiAgICAgICAgLnNlbGVjdCgiZGVwYXJ0YW1lbnRvIiwgInZlaGljdWxvc19wb3JfYWNjaWRlbnRlIiwgInZlaGljdWxvc190b3QiLCAiYWNjaWRlbnRlc190b3QiKQogICAgICAgIC5vcmRlckJ5KEYuZGVzYygidmVoaWN1bG9zX3Bvcl9hY2NpZGVudGUiKSkKKQoKZGlzcGxheShwcm9tX2RlcHRvKSAgIyB2aXN0YSBwcmV2aWEKCiMgR3VhcmRhciBjb21vIFBhcnF1ZXRlCk9VVF9ESVIgPSAiL1ZvbHVtZXMvd29ya3NwYWNlL2RlZmF1bHQvbGFiOC9vdXQiCk9VVF9QQVRIID0gZiJ7T1VUX0RJUn0vcHJvbV92ZWhpY3Vsb3NfYWNjaWRlbnRlIgooCiAgICBwcm9tX2RlcHRvCiAgICAuY29hbGVzY2UoMSkgICAgICAgICAgICAgIAogICAgLndyaXRlLm1vZGUoIm92ZXJ3cml0ZSIpCiAgICAucGFycXVldChPVVRfUEFUSCkKKQpwcmludChmIkd1YXJkYWRvIFBhcnF1ZXQgZW46IHtPVVRfUEFUSH0iKQoKIyBSZWNhcmdhciBkZXNkZSBQYXJxdWV0CnByb21fcmVsb2FkID0gc3BhcmsucmVhZC5wYXJxdWV0KE9VVF9QQVRIKQoKIyBUb3AgMTAgZGVwYXJ0YW1lbnRvcyBjb24gbcOhcyB2ZWjDrWN1bG9zL2FjY2lkZW50ZQp0b3AxMCA9IHByb21fcmVsb2FkLm9yZGVyQnkoRi5kZXNjKCJ2ZWhpY3Vsb3NfcG9yX2FjY2lkZW50ZSIpKS5saW1pdCgxMCkKZGlzcGxheSh0b3AxMCkK\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 1:\n        # create a temp view\n        if type(__backend_agg_dfs[1]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[1].to_spark().createOrReplaceTempView(\"DatabricksViewd6865b4\")\n        elif type(__backend_agg_dfs[1]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[1], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[1]).createOrReplaceTempView(\"DatabricksViewd6865b4\")\n        else:\n            __backend_agg_dfs[1].createOrReplaceTempView(\"DatabricksViewd6865b4\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewd6865b4) SELECT `departamento`,AVG(`vehiculos_por_accidente`) `column_36ff4f50512` FROM q GROUP BY `departamento`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewd6865b4\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "departamento",
             "id": "column_36ff4f50510"
            },
            "y": [
             {
              "column": "vehiculos_por_accidente",
              "id": "column_36ff4f50512",
              "transform": "AVG"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_36ff4f50512": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "title": {
              "text": "Vehiculos por accidente"
             },
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "0c8973af-c856-479c-9d1b-11a8fe58b0a4",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 33.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "departamento",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "departamento",
           "type": "column"
          },
          {
           "alias": "column_36ff4f50512",
           "args": [
            {
             "column": "vehiculos_por_accidente",
             "type": "column"
            }
           ],
           "function": "AVG",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 1,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================== 9) Promedio de vehículos por accidente por departamento ==================\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Partimos del join del inciso 8\n",
    "assert 'HV' in globals(), \"No encuentro 'HV'. Ejecuta primero el inciso 8.\"\n",
    "hvdf = HV\n",
    "\n",
    "# Agregar por departamento: promedio = sum(vehiculos) / sum(accidentes)\n",
    "prom_depto = (\n",
    "    hvdf.groupBy(\"departamento\")\n",
    "        .agg(\n",
    "            F.sum(\"vehiculos\").alias(\"vehiculos_tot\"),\n",
    "            F.sum(\"accidentes\").alias(\"accidentes_tot\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"vehiculos_por_accidente\",\n",
    "            F.when(F.col(\"accidentes_tot\") > 0, F.col(\"vehiculos_tot\")/F.col(\"accidentes_tot\"))\n",
    "             .otherwise(F.lit(None))\n",
    "        )\n",
    "        .select(\"departamento\", \"vehiculos_por_accidente\", \"vehiculos_tot\", \"accidentes_tot\")\n",
    "        .orderBy(F.desc(\"vehiculos_por_accidente\"))\n",
    ")\n",
    "\n",
    "display(prom_depto)  # vista previa\n",
    "\n",
    "# Guardar como Parquete\n",
    "OUT_DIR = \"/Volumes/workspace/default/lab8/out\"\n",
    "OUT_PATH = f\"{OUT_DIR}/prom_vehiculos_accidente\"\n",
    "(\n",
    "    prom_depto\n",
    "    .coalesce(1)              \n",
    "    .write.mode(\"overwrite\")\n",
    "    .parquet(OUT_PATH)\n",
    ")\n",
    "print(f\"Guardado Parquet en: {OUT_PATH}\")\n",
    "\n",
    "# Recargar desde Parquet\n",
    "prom_reload = spark.read.parquet(OUT_PATH)\n",
    "\n",
    "# Top 10 departamentos con más vehículos/accidente\n",
    "top10 = prom_reload.orderBy(F.desc(\"vehiculos_por_accidente\")).limit(10)\n",
    "display(top10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a54341c1-4d24-4a58-8353-ca8b0d2a2a9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================== 10) Top 5 colores de vehiculos con más accidentes ==================\n",
    "(vehiculos\n",
    " .filter(F.col(\"color_veh\").isNotNull() & (F.length(F.trim(F.col(\"color_veh\"))) > 0))\n",
    " .filter(~F.upper(F.col(\"color_veh\")).isin(\"IGNORADO\"))\n",
    " .groupBy(\"color_veh\")\n",
    " .agg(F.count(F.lit(1)).alias(\"accidentes\"))\n",
    " .orderBy(F.desc(\"accidentes\"))\n",
    " .limit(5)\n",
    " .show(truncate=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a850a74-76ff-4c8c-8743-09ab1b07389c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================== 11) Lesionados por atropello 2024 - por mes ==================\n",
    "\n",
    "falle_2024 = (fallecidos_lesionados\n",
    "    .withColumn(\"ano_int\", F.expr(\"try_cast(ano_ocu as int)\"))\n",
    "    .withColumn(\"tipo_eve_lc\", F.lower(F.col(\"tipo_eve\")))\n",
    "    .withColumn(\"fall_les_lc\", F.lower(F.col(\"fall_les\")))\n",
    "    .withColumn(\"mes_lc\", F.lower(F.col(\"mes_ocu\")))\n",
    "    .filter( (F.col(\"ano_int\")==2023) &\n",
    "             (F.col(\"tipo_eve_lc\")==\"atropello\") &\n",
    "             (F.col(\"fall_les_lc\")==\"lesionado\") )\n",
    ")\n",
    "\n",
    "month_order = {\n",
    "    \"enero\":1, \"febrero\":2, \"marzo\":3, \"abril\":4, \"mayo\":5, \"junio\":6,\n",
    "    \"julio\":7, \"agosto\":8, \"septiembre\":9, \"octubre\":10, \"noviembre\":11, \"diciembre\":12\n",
    "}\n",
    "month_map = F.create_map([x for kv in month_order.items() for x in (F.lit(kv[0]), F.lit(kv[1]))])\n",
    "\n",
    "lesiones_mensual = (falle_2024\n",
    "    .groupBy(\"mes_lc\")\n",
    "    .agg(F.count(F.lit(1)).alias(\"lesionados\"))\n",
    "    .withColumn(\"mes_num\", F.element_at(month_map, F.col(\"mes_lc\")))\n",
    "    .orderBy(\"mes_num\")\n",
    "    .select(F.col(\"mes_lc\").alias(\"mes\"), \"lesionados\")\n",
    ")\n",
    "\n",
    "display(lesiones_mensual)\n",
    "\n",
    "# Gráfica\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pdf = lesiones_mensual.toPandas()\n",
    "pdf = pdf.sort_values(by=\"mes\", key=lambda s: s.map(month_order))\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(pdf[\"mes\"], pdf[\"lesionados\"], marker=\"o\")\n",
    "plt.title(\"Lesionados por atropello en 2024 (por mes)\")\n",
    "plt.xlabel(\"Mes\")\n",
    "plt.ylabel(\"Cantidad de lesionados\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8291368-0de0-46a9-9ae6-f39a9b030429",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================== 12) Fallecidos por tipo de accidente ==================\n",
    "\n",
    "# Unificar por llave\n",
    "fl_src = spark.read.csv(FALLE_GLOB, header=True, inferSchema=False)\n",
    "\n",
    "def unify_cols_fl(df):\n",
    "    ren = {}\n",
    "    if \"departamento\" not in df.columns and \"depto_ocu\" in df.columns:\n",
    "        ren[\"depto_ocu\"] = \"departamento\"\n",
    "    if \"anio\" not in df.columns:\n",
    "        if \"ano_ocu\" in df.columns: ren[\"ano_ocu\"] = \"anio_raw\"\n",
    "    if \"mes\" not in df.columns:\n",
    "        if \"mes_ocu\" in df.columns: ren[\"mes_ocu\"] = \"mes_raw\"\n",
    "    if \"tipo_accidente\" not in df.columns:\n",
    "        if \"tipo_eve\" in df.columns: ren[\"tipo_eve\"] = \"tipo_raw\"\n",
    "    out = df\n",
    "    for k,v in ren.items():\n",
    "        out = out.withColumnRenamed(k, v)\n",
    "    return out\n",
    "\n",
    "fl = unify_cols_fl(fl_src)\n",
    "\n",
    "fl = (fl\n",
    "      .withColumn(\"departamento\",   clean_depto(\"departamento\"))\n",
    "      .withColumn(\"anio\",           clean_anio(pick(fl, \"anio_raw\", \"anio\")))\n",
    "      .withColumn(\"mes\",            clean_mes(pick(fl, \"mes_raw\",  \"mes\")))\n",
    "      .withColumn(\"tipo_accidente\", clean_tipo(pick(fl, \"tipo_raw\",\"tipo_accidente\")))\n",
    "      .withColumn(\"fall_les_norm\",  F.upper(F.trim(F.col(\"fall_les\"))))\n",
    "      .select(\"anio\",\"mes\",\"departamento\",\"tipo_accidente\",\"fall_les_norm\")\n",
    ")\n",
    "\n",
    "VALID_MES = r\"^(0[1-9]|1[0-2])$\"\n",
    "fl_k = fl.filter(\n",
    "    (F.col(\"anio\").isNotNull()) &\n",
    "    (F.col(\"mes\").rlike(VALID_MES)) &\n",
    "    (F.col(\"departamento\").isNotNull()) &\n",
    "    (F.col(\"tipo_accidente\").isNotNull())\n",
    ")\n",
    "\n",
    "FL = (fl_k\n",
    "      .groupBy(\"anio\",\"mes\",\"departamento\",\"tipo_accidente\")\n",
    "      .agg(\n",
    "          F.sum(F.when(F.col(\"fall_les_norm\")==\"FALLECIDO\", 1).otherwise(0)).alias(\"fallecidos\"),\n",
    "          F.sum(F.when(F.col(\"fall_les_norm\")==\"LESIONADO\", 1).otherwise(0)).alias(\"lesionados\")\n",
    "      )\n",
    "      .withColumn(\"total_fl\", F.col(\"fallecidos\")+F.col(\"lesionados\"))\n",
    ")\n",
    "\n",
    "HV_FL = (HV.join(FL, on=[\"anio\",\"mes\",\"departamento\",\"tipo_accidente\"], how=\"left\")\n",
    "           .fillna({\"fallecidos\":0, \"lesionados\":0, \"total_fl\":0})\n",
    ")\n",
    "\n",
    "print(f\"Registros combinados (H⨝V⨝FL): {HV_FL.count():,}\")\n",
    "display(HV_FL.orderBy(\"anio\",\"mes\",\"departamento\",\"tipo_accidente\").limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18c7049a-b69c-43d4-9604-ff246ee117cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fallecidos por accidente\n",
    "fallecidos_por_tipo = (\n",
    "    HV_FL.groupBy(\"tipo_accidente\")\n",
    "         .agg(F.sum(F.col(\"fallecidos\")).alias(\"total_fallecidos\"))\n",
    "         .orderBy(F.desc(\"total_fallecidos\"))\n",
    ")\n",
    "\n",
    "display(fallecidos_por_tipo)\n",
    "\n",
    "# Gráfica\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pdf = fallecidos_por_tipo.toPandas()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(pdf[\"tipo_accidente\"], pdf[\"total_fallecidos\"])\n",
    "plt.xlabel(\"Total de fallecidos\")\n",
    "plt.ylabel(\"Tipo de accidente\")\n",
    "plt.title(\"Fallecidos por tipo de accidente\")\n",
    "plt.gca().invert_yaxis()  # para que el mayor quede arriba\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dbc815e-371a-4719-a672-27ec813a9009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================== 13) Clasificación de accidentes por franja horaria ==================\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Limpiamos y convertimos la hora a entero si no lo está\n",
    "hechos_hora = (\n",
    "    hechos\n",
    "    .withColumn(\"hora_int\", F.when(F.col(\"hora_ocu\").rlike(\"^[0-9]+$\"), F.col(\"hora_ocu\").cast(\"int\")).otherwise(None))\n",
    "    .filter(F.col(\"hora_int\").isNotNull())\n",
    ")\n",
    "\n",
    "# Clasificamos las franjas horarias\n",
    "hechos_franjas = (\n",
    "    hechos_hora\n",
    "    .withColumn(\n",
    "        \"franja_horaria\",\n",
    "        F.when((F.col(\"hora_int\") >= 6) & (F.col(\"hora_int\") < 12), \"Mañana\")\n",
    "         .when((F.col(\"hora_int\") >= 12) & (F.col(\"hora_int\") < 18), \"Tarde\")\n",
    "         .when((F.col(\"hora_int\") >= 18) & (F.col(\"hora_int\") < 24), \"Noche\")\n",
    "         .when((F.col(\"hora_int\") >= 0)  & (F.col(\"hora_int\") < 6),  \"Madrugada\")\n",
    "         .otherwise(\"Desconocido\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Agrupamos para contar accidentes por franja\n",
    "acc_por_franja = (\n",
    "    hechos_franjas\n",
    "    .groupBy(\"franja_horaria\")\n",
    "    .agg(F.count(\"*\").alias(\"accidentes\"))\n",
    "    .orderBy(F.desc(\"accidentes\"))\n",
    ")\n",
    "\n",
    "display(acc_por_franja)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9e38093-a3a8-4ef6-b024-45fa418aab38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================== 14) Ratio fallecidos/accidente por departamento ==================\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "FALLECIDOS_GLOB = \"/Volumes/workspace/default/lab8/csv/fallecidos-lesionados/*.csv\"\n",
    "OUT_PATH        = \"/Volumes/workspace/default/lab8/resultados/ratio_fallecidos_por_accidente\"\n",
    "\n",
    "# Helpers de limpieza\n",
    "def clean_depto(col):\n",
    "    c = F.upper(F.trim(F.col(col)))\n",
    "    c = F.translate(c, \"ÁÉÍÓÚáéíóúÑñ\", \"AEIOUaeiouNn\")\n",
    "    c = F.regexp_replace(c, r\"\\?\", \"E\")\n",
    "    c = F.regexp_replace(c, r\"\\s+\", \" \")\n",
    "    return c\n",
    "\n",
    "# Base de accidentes (hechos) por departamento\n",
    "hec = hechos.withColumnRenamed(\"depto_ocu\",\"departamento\") if \"depto_ocu\" in hechos.columns else hechos\n",
    "hec = hec.withColumn(\"departamento\", clean_depto(\"departamento\"))\n",
    "\n",
    "acc_dep = (hec\n",
    "    .groupBy(\"departamento\")\n",
    "    .agg(F.count(F.lit(1)).alias(\"total_accidentes\"))\n",
    ")\n",
    "\n",
    "# Víctimas: detectar fallecidos desde `fall_les`\n",
    "vic = spark.read.csv(FALLECIDOS_GLOB, header=True, inferSchema=False)\n",
    "vic = vic.withColumnRenamed(\"depto_ocu\",\"departamento\") if \"depto_ocu\" in vic.columns else vic\n",
    "vic = vic.withColumn(\"departamento\", clean_depto(\"departamento\"))\n",
    "\n",
    "# Normaliza texto de fall_les\n",
    "fall = F.upper(F.trim(F.col(\"fall_les\"))) if \"fall_les\" in vic.columns else F.lit(None)\n",
    "\n",
    "fallecido_flag = F.when(\n",
    "    fall.rlike(r\"^FALLEC\") | (fall == F.lit(\"F\")) |\n",
    "    (F.when(F.col(\"fall_les\").rlike(r\"^\\d+$\"), F.col(\"fall_les\").cast(\"int\")).otherwise(F.lit(0)) > 0),\n",
    "    F.lit(1)\n",
    ").otherwise(F.lit(0))\n",
    "\n",
    "vic_dep = (vic\n",
    "    .withColumn(\"fallecido\", fallecido_flag)\n",
    "    .groupBy(\"departamento\")\n",
    "    .agg(F.sum(\"fallecido\").alias(\"total_fallecidos\"))\n",
    ")\n",
    "\n",
    "# Ratio por departamento\n",
    "ratio_df = (acc_dep.join(vic_dep, on=\"departamento\", how=\"left\")\n",
    "    .fillna({\"total_fallecidos\": 0})\n",
    "    .withColumn(\"ratio_fallecidos\",\n",
    "                F.round(F.col(\"total_fallecidos\") / F.col(\"total_accidentes\"), 4))\n",
    "    .orderBy(F.desc(\"ratio_fallecidos\"))\n",
    ")\n",
    "\n",
    "display(ratio_df)\n",
    "\n",
    "# Guardar en Parquet y recargar top-10 para graficar con display\n",
    "ratio_df.write.mode(\"overwrite\").parquet(OUT_PATH)\n",
    "print(f\"Guardado en: {OUT_PATH}\")\n",
    "\n",
    "top10 = spark.read.parquet(OUT_PATH).orderBy(F.desc(\"ratio_fallecidos\")).limit(10)\n",
    "display(top10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b536f35b-b3cd-4791-a528-36352e76c633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================== 15) Barras comparativas FALLECIDO vs LESIONADO por grupo de edad ==================\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "VIC_GLOB = \"/Volumes/workspace/default/lab8/csv/fallecidos-lesionados/*.csv\"\n",
    "vic = spark.read.csv(VIC_GLOB, header=True, inferSchema=False)\n",
    "\n",
    "cands = [c for c in vic.columns if \"edad\" in c.lower()]\n",
    "edad_col = None\n",
    "for c in cands:\n",
    "    if vic.filter(F.col(c).cast(\"string\").rlike(r\"\\d+\")).limit(1).count() > 0:\n",
    "        edad_col = c\n",
    "        break\n",
    "if edad_col is None:\n",
    "    raise ValueError(\"No se encontró una columna de edad utilizable en el CSV.\")\n",
    "\n",
    "print(\"Columna elegida para edad:\", edad_col)\n",
    "\n",
    "fall = F.upper(F.trim(F.col(\"fall_les\").cast(\"string\")))\n",
    "flag = F.when(F.col(\"fall_les\").rlike(r\"^\\d+$\"), F.col(\"fall_les\").cast(\"int\"))\n",
    "estado = (\n",
    "    F.when((fall.rlike(r\"^FALLEC\") | (fall==\"F\") | (flag==1)),  \"FALLECIDO\")\n",
    "     .when((fall.rlike(r\"^LESION\") | (fall==\"L\") | (flag==0)), \"LESIONADO\")\n",
    "     .otherwise(\"DESCONOCIDO\")\n",
    ")\n",
    "\n",
    "digits   = F.regexp_extract(F.col(edad_col).cast(\"string\"), r\"\\d+\", 0)\n",
    "edad_int = F.when(digits==\"\", None).otherwise(digits.cast(\"int\"))\n",
    "\n",
    "grupo = (\n",
    "    F.when((edad_int>=0)  & (edad_int<=11), \"00–11\")\n",
    "     .when((edad_int>=12) & (edad_int<=17), \"12–17\")\n",
    "     .when((edad_int>=18) & (edad_int<=29), \"18–29\")\n",
    "     .when((edad_int>=30) & (edad_int<=44), \"30–44\")\n",
    "     .when((edad_int>=45) & (edad_int<=59), \"45–59\")\n",
    "     .when((edad_int>=60),                  \"60+\")\n",
    "     .otherwise(\"SIN_EDAD\")\n",
    ")\n",
    "\n",
    "vic_bins = (vic\n",
    "    .withColumn(\"estado\", estado)\n",
    "    .withColumn(\"edad\",   edad_int)\n",
    "    .withColumn(\"grupo_edad\", grupo)\n",
    ")\n",
    "\n",
    "orden = (F.when(F.col(\"grupo_edad\")==\"00–11\",0)\n",
    "          .when(F.col(\"grupo_edad\")==\"12–17\",1)\n",
    "          .when(F.col(\"grupo_edad\")==\"18–29\",2)\n",
    "          .when(F.col(\"grupo_edad\")==\"30–44\",3)\n",
    "          .when(F.col(\"grupo_edad\")==\"45–59\",4)\n",
    "          .when(F.col(\"grupo_edad\")==\"60+\",  5)\n",
    "          .otherwise(6))\n",
    "\n",
    "res = (vic_bins\n",
    "    .filter(F.col(\"estado\").isin(\"FALLECIDO\",\"LESIONADO\"))\n",
    "    .groupBy(\"grupo_edad\",\"estado\")\n",
    "    .agg(F.count(F.lit(1)).alias(\"personas\"))\n",
    "    .withColumn(\"orden\", orden)\n",
    "    .filter(F.col(\"grupo_edad\") != \"SIN_EDAD\")\n",
    "    .orderBy(\"orden\",\"estado\")\n",
    ")\n",
    "\n",
    "display(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8be11b2-4475-4e56-ae83-9798f70ab055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pdf = res.toPandas()\n",
    "\n",
    "pivot = pdf.pivot(index='grupo_edad', columns='estado', values='personas').fillna(0)\n",
    "\n",
    "# Ordenar correctamente los grupos de edad\n",
    "orden = [\"00–11\", \"12–17\", \"18–29\", \"30–44\", \"45–59\", \"60+\"]\n",
    "pivot = pivot.reindex(orden)\n",
    "\n",
    "# Crear gráfico de barras comparativas\n",
    "ax = pivot.plot(kind='bar', figsize=(8,5), width=0.8)\n",
    "\n",
    "plt.title(\"Comparativa de fallecidos y lesionados por grupo de edad\")\n",
    "plt.xlabel(\"Grupo de edad\")\n",
    "plt.ylabel(\"Número de personas\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title=\"Estado\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d089437-a2ad-4f70-8f3d-a2412e0b3432",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================== 16) Accidentes y fallecidos por zona (Municipio GUATEMALA) ==================\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "HECHOS_GLOB = \"/Volumes/workspace/default/lab8/csv/hechos/*.csv\"\n",
    "VIC_GLOB    = \"/Volumes/workspace/default/lab8/csv/fallecidos-lesionados/*.csv\"\n",
    "\n",
    "hec_src = spark.read.csv(HECHOS_GLOB, header=True, inferSchema=False)\n",
    "vic_src = spark.read.csv(VIC_GLOB,   header=True, inferSchema=False)\n",
    "\n",
    "def alias(df):\n",
    "    ren = {}\n",
    "    if \"depto_ocu\" in df.columns and \"departamento\" not in df.columns: ren[\"depto_ocu\"] = \"departamento\"\n",
    "    if \"ano_ocu\"   in df.columns and \"anio\"         not in df.columns: ren[\"ano_ocu\"]   = \"anio\"\n",
    "    if \"mes_ocu\"   in df.columns and \"mes\"          not in df.columns: ren[\"mes_ocu\"]   = \"mes\"\n",
    "    if \"tipo_eve\"  in df.columns and \"tipo_accidente\" not in df.columns: ren[\"tipo_eve\"] = \"tipo_accidente\"\n",
    "    if \"zona_ocu\"  in df.columns and \"zona\"         not in df.columns: ren[\"zona_ocu\"]  = \"zona\"\n",
    "    out = df\n",
    "    for k,v in ren.items():\n",
    "        out = out.withColumnRenamed(k, v)\n",
    "    return out\n",
    "\n",
    "hec = alias(hec_src)\n",
    "vic = alias(vic_src)\n",
    "\n",
    "def clean_depto(c):\n",
    "    return F.regexp_replace(\n",
    "        F.translate(F.upper(F.trim(F.col(c))), \"ÁÉÍÓÚáéíóúÑñ\", \"AEIOUaeiouNn\"),\n",
    "        r\"\\?\", \"E\"\n",
    "    )\n",
    "\n",
    "MESES = {\"ENERO\":\"01\",\"FEBRERO\":\"02\",\"MARZO\":\"03\",\"ABRIL\":\"04\",\"MAYO\":\"05\",\"JUNIO\":\"06\",\n",
    "         \"JULIO\":\"07\",\"AGOSTO\":\"08\",\"SEPTIEMBRE\":\"09\",\"SETIEMBRE\":\"09\",\"OCTUBRE\":\"10\",\n",
    "         \"NOVIEMBRE\":\"11\",\"DICIEMBRE\":\"12\"}\n",
    "\n",
    "def clean_mes(c):\n",
    "    m = F.translate(F.upper(F.trim(F.col(c))), \"ÁÉÍÓÚáéíóú\", \"AEIOUaeiou\")\n",
    "    expr = None\n",
    "    for k,v in MESES.items():\n",
    "        expr = F.when(m==k, v) if expr is None else expr.when(m==k, v)\n",
    "    num = F.lpad(F.regexp_extract(m, r\"\\d+\", 0), 2, \"0\")\n",
    "    return (expr.otherwise(num) if expr is not None else num)\n",
    "\n",
    "def clean_anio(c):  return F.regexp_extract(F.col(c).cast(\"string\"), r\"\\d{4}\", 0).cast(\"int\")\n",
    "def clean_tipo(c):  return F.upper(F.trim(F.col(c)))\n",
    "\n",
    "hec = (hec\n",
    "    .withColumn(\"departamento\", clean_depto(\"departamento\"))\n",
    "    .withColumn(\"mes\",          clean_mes(\"mes\"))\n",
    "    .withColumn(\"anio\",         clean_anio(\"anio\"))\n",
    "    .withColumn(\"tipo_accidente\", clean_tipo(\"tipo_accidente\"))\n",
    ")\n",
    "\n",
    "vic = (vic\n",
    "    .withColumn(\"departamento\", clean_depto(\"departamento\"))\n",
    "    .withColumn(\"mes\",          clean_mes(\"mes\"))\n",
    "    .withColumn(\"anio\",         clean_anio(\"anio\"))\n",
    "    .withColumn(\"tipo_accidente\", clean_tipo(\"tipo_accidente\"))\n",
    "    .withColumn(\"zona_num\", F.regexp_extract(F.upper(F.trim(F.col(\"zona\"))), r\"\\d+\", 0))\n",
    ")\n",
    "\n",
    "VALID_MES = r\"^(0[1-9]|1[0-2])$\"\n",
    "hec_k = hec.filter( F.col(\"anio\").isNotNull() & F.col(\"mes\").rlike(VALID_MES)\n",
    "                    & F.col(\"departamento\").isNotNull() & F.col(\"tipo_accidente\").isNotNull() )\n",
    "\n",
    "vic_k = vic.filter( F.col(\"anio\").isNotNull() & F.col(\"mes\").rlike(VALID_MES)\n",
    "                    & F.col(\"departamento\").isNotNull() & F.col(\"tipo_accidente\").isNotNull()\n",
    "                    & (F.col(\"zona_num\")!=\"\") )\n",
    "\n",
    "# Accidentes por llave (en GUATEMALA)\n",
    "H = (hec_k.filter(F.col(\"departamento\")==\"GUATEMALA\")\n",
    "        .groupBy(\"anio\",\"mes\",\"departamento\",\"tipo_accidente\")\n",
    "        .agg(F.count(F.lit(1)).alias(\"accidentes\"))\n",
    ")\n",
    "\n",
    "# Llave + zona desde víctimas (en GUATEMALA)\n",
    "KZ = (vic_k.filter(F.col(\"departamento\")==\"GUATEMALA\")\n",
    "         .select(\"anio\",\"mes\",\"departamento\",\"tipo_accidente\",\"zona_num\")\n",
    "         .distinct()\n",
    ")\n",
    "\n",
    "# Accidentes por zona (usando la zona traída de víctimas)\n",
    "acc_zona = (H.join(KZ, on=[\"anio\",\"mes\",\"departamento\",\"tipo_accidente\"], how=\"inner\")\n",
    "              .groupBy(\"zona_num\")\n",
    "              .agg(F.sum(\"accidentes\").alias(\"accidentes\")))\n",
    "\n",
    "# Fallecidos por zona\n",
    "fall = F.upper(F.trim(F.col(\"fall_les\")))\n",
    "flag = F.when(F.col(\"fall_les\").rlike(r\"^\\d+$\"), F.col(\"fall_les\").cast(\"int\"))\n",
    "es_fallecido = (fall.rlike(r\"^FALLEC\") | (fall==\"F\") | (flag==1))\n",
    "\n",
    "fall_zona = (vic_k.filter( (F.col(\"departamento\")==\"GUATEMALA\") & es_fallecido )\n",
    "                .groupBy(\"zona_num\")\n",
    "                .agg(F.count(F.lit(1)).alias(\"fallecidos\")))\n",
    "\n",
    "# Unir indicadores y ordenar por zona numérica\n",
    "zona_ind = (acc_zona.join(fall_zona, on=\"zona_num\", how=\"outer\")\n",
    "                     .na.fill(0)\n",
    "                     .withColumnRenamed(\"zona_num\",\"zona\")\n",
    "                     .orderBy(F.col(\"zona\").cast(\"int\")))\n",
    "\n",
    "display(zona_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f71013c-0fbe-4c5d-9c3e-a96d319dbd77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pdf = zona_ind.toPandas()\n",
    "pdf[\"zona\"] = pdf[\"zona\"].astype(int)\n",
    "pdf = pdf.sort_values(\"zona\")\n",
    "\n",
    "ax = (\n",
    "    pdf.set_index(\"zona\")[[\"accidentes\", \"fallecidos\"]]\n",
    "       .plot(kind=\"bar\", figsize=(11,5), width=0.85)\n",
    ")\n",
    "\n",
    "plt.title(\"Municipio de Guatemala: Accidentes vs Fallecidos por zona\")\n",
    "plt.xlabel(\"Zona\")\n",
    "plt.ylabel(\"Cantidad\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title=\"Indicador\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afc7aff4-999a-4c12-ae6c-94600805ec6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================== 17) Porcentaje de accidentes por sexo del conductor ==================\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "VEH_GLOB = \"/Volumes/workspace/default/lab8/csv/vehiculos/*.csv\"\n",
    "\n",
    "veh = spark.read.csv(VEH_GLOB, header=True, inferSchema=False)\n",
    "\n",
    "# Detectar columna de sexo\n",
    "cands = [c for c in veh.columns if \"sexo\" in c.lower()]\n",
    "sexo_col = None\n",
    "for c in cands:\n",
    "    if veh.filter(F.col(c).isNotNull()).limit(1).count() > 0:\n",
    "        sexo_col = c\n",
    "        break\n",
    "\n",
    "if sexo_col is None:\n",
    "    raise ValueError(\"No se encontró una columna de sexo utilizable en los CSV de vehículos.\")\n",
    "\n",
    "print(\"Columna elegida para sexo:\", sexo_col)\n",
    "\n",
    "# Normalizar los valores\n",
    "sexo_norm = (\n",
    "    F.when(F.upper(F.trim(F.col(sexo_col))).isin(\"M\",\"MASCULINO\",\"HOMBRE\"), \"HOMBRE\")\n",
    "     .when(F.upper(F.trim(F.col(sexo_col))).isin(\"F\",\"FEMENINO\",\"MUJER\"), \"MUJER\")\n",
    "     .otherwise(\"DESCONOCIDO\")\n",
    ")\n",
    "\n",
    "veh_norm = veh.withColumn(\"sexo_norm\", sexo_norm)\n",
    "\n",
    "# Calcular porcentajes\n",
    "total = veh_norm.count()\n",
    "res = (\n",
    "    veh_norm.groupBy(\"sexo_norm\")\n",
    "    .agg(F.count(F.lit(1)).alias(\"accidentes\"))\n",
    "    .withColumn(\"porcentaje\", F.round(F.col(\"accidentes\") * 100 / total, 2))\n",
    "    .filter(F.col(\"sexo_norm\") != \"DESCONOCIDO\")\n",
    ")\n",
    "\n",
    "# Guardar en Parquet\n",
    "OUT_PATH = \"/Volumes/workspace/default/lab8/parquet/accidentes_por_sexo\"\n",
    "res.write.mode(\"overwrite\").parquet(OUT_PATH)\n",
    "\n",
    "# Volver a cargar y mostrar\n",
    "res_loaded = spark.read.parquet(OUT_PATH)\n",
    "display(res_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7896f40-2e3e-4d73-86e8-d81575209fde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pdf = res_loaded.toPandas().sort_values(\"sexo_norm\")\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(pdf[\"porcentaje\"], labels=pdf[\"sexo_norm\"], autopct='%1.1f%%', startangle=90)\n",
    "plt.title(\"Porcentaje de accidentes según sexo del conductor\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "laboratorio8",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
